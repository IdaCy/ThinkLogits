{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/ThinkLogits\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/ThinkLogits/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "\n",
    "from src.main.pipeline import load_model_and_tokenizer, generate_dataset_completions\n",
    "from src.eval.llm_verificator import run_verification\n",
    "from src.eval.switch_check import run_switch_check\n",
    "from src.eval.llm_hint_verificator import run_hint_verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 14:13:31,853 - INFO - CUDA is available. Using GPU.\n",
      "2025-04-15 14:13:31,854 - INFO - Loading model and tokenizer: deepseek-ai/DeepSeek-R1-Distill-Llama-8B onto cuda\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 48.67it/s]\n",
      "2025-04-15 14:13:36,486 - INFO - Model and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# model_path = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "model_path = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "model, tokenizer, model_name, device = load_model_and_tokenizer(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"mmlu\"\n",
    "hint_types = [\"none\", \"sycophancy\", \"unethical_information\", \"induced_urgency\"]\n",
    "n_questions = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 14:14:09,237 - INFO - Using chat template: <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "2025-04-15 14:14:09,238 - INFO - --- Processing dataset for hint type: none ---\n",
      "2025-04-15 14:14:09,253 - ERROR - Data file not found: data/mmlu/hints_none.json\n",
      "2025-04-15 14:14:09,254 - INFO - Generating completions for none...\n",
      "2025-04-15 14:14:09,255 - INFO - Using max_new_tokens: 2048\n",
      "2025-04-15 14:14:09,255 - INFO - Processing batch 1/10 (Size: 50, QIDs: 0-49)\n",
      "/home/ubuntu/ThinkLogits/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/ThinkLogits/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:17:11,520 - INFO - Processing batch 2/10 (Size: 50, QIDs: 50-99)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:20:17,947 - INFO - Processing batch 3/10 (Size: 50, QIDs: 100-149)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:23:19,476 - INFO - Processing batch 4/10 (Size: 50, QIDs: 150-199)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:26:13,134 - INFO - Processing batch 5/10 (Size: 50, QIDs: 200-249)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:29:14,553 - INFO - Processing batch 6/10 (Size: 50, QIDs: 250-299)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:32:02,665 - INFO - Processing batch 7/10 (Size: 50, QIDs: 300-349)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:34:52,638 - INFO - Processing batch 8/10 (Size: 50, QIDs: 350-399)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:38:06,339 - INFO - Processing batch 9/10 (Size: 50, QIDs: 400-449)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:41:07,916 - INFO - Processing batch 10/10 (Size: 50, QIDs: 450-499)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:44:20,746 - INFO - Results saved to data/mmlu/DeepSeek-R1-Distill-Llama-8B/none/completions_with_500.json\n",
      "2025-04-15 14:44:20,747 - INFO - --- Processing dataset for hint type: sycophancy ---\n",
      "2025-04-15 14:44:20,764 - INFO - Generating completions for sycophancy...\n",
      "2025-04-15 14:44:20,766 - INFO - Using max_new_tokens: 2048\n",
      "2025-04-15 14:44:20,766 - INFO - Processing batch 1/10 (Size: 50, QIDs: 0-49)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:47:24,011 - INFO - Processing batch 2/10 (Size: 50, QIDs: 50-99)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:50:32,499 - INFO - Processing batch 3/10 (Size: 50, QIDs: 100-149)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:53:35,964 - INFO - Processing batch 4/10 (Size: 50, QIDs: 150-199)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:56:31,565 - INFO - Processing batch 5/10 (Size: 50, QIDs: 200-249)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:59:35,011 - INFO - Processing batch 6/10 (Size: 50, QIDs: 250-299)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:02:25,009 - INFO - Processing batch 7/10 (Size: 50, QIDs: 300-349)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:05:16,926 - INFO - Processing batch 8/10 (Size: 50, QIDs: 350-399)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:08:33,341 - INFO - Processing batch 9/10 (Size: 50, QIDs: 400-449)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:11:38,125 - INFO - Processing batch 10/10 (Size: 50, QIDs: 450-499)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:14:52,577 - INFO - Results saved to data/mmlu/DeepSeek-R1-Distill-Llama-8B/sycophancy/completions_with_500.json\n",
      "2025-04-15 15:14:52,578 - INFO - --- Processing dataset for hint type: unethical_information ---\n",
      "2025-04-15 15:14:52,595 - INFO - Generating completions for unethical_information...\n",
      "2025-04-15 15:14:52,596 - INFO - Using max_new_tokens: 2048\n",
      "2025-04-15 15:14:52,597 - INFO - Processing batch 1/10 (Size: 50, QIDs: 0-49)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:17:56,827 - INFO - Processing batch 2/10 (Size: 50, QIDs: 50-99)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:21:05,836 - INFO - Processing batch 3/10 (Size: 50, QIDs: 100-149)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:24:10,057 - INFO - Processing batch 4/10 (Size: 50, QIDs: 150-199)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:27:06,025 - INFO - Processing batch 5/10 (Size: 50, QIDs: 200-249)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:30:10,178 - INFO - Processing batch 6/10 (Size: 50, QIDs: 250-299)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:33:00,733 - INFO - Processing batch 7/10 (Size: 50, QIDs: 300-349)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:35:52,899 - INFO - Processing batch 8/10 (Size: 50, QIDs: 350-399)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:39:09,661 - INFO - Processing batch 9/10 (Size: 50, QIDs: 400-449)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:42:13,836 - INFO - Processing batch 10/10 (Size: 50, QIDs: 450-499)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:45:29,297 - INFO - Results saved to data/mmlu/DeepSeek-R1-Distill-Llama-8B/unethical_information/completions_with_500.json\n",
      "2025-04-15 15:45:29,298 - INFO - --- Processing dataset for hint type: induced_urgency ---\n",
      "2025-04-15 15:45:29,313 - INFO - Generating completions for induced_urgency...\n",
      "2025-04-15 15:45:29,315 - INFO - Using max_new_tokens: 2048\n",
      "2025-04-15 15:45:29,315 - INFO - Processing batch 1/10 (Size: 50, QIDs: 0-49)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:48:33,937 - INFO - Processing batch 2/10 (Size: 50, QIDs: 50-99)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:51:42,932 - INFO - Processing batch 3/10 (Size: 50, QIDs: 100-149)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:54:46,961 - INFO - Processing batch 4/10 (Size: 50, QIDs: 150-199)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:57:42,954 - INFO - Processing batch 5/10 (Size: 50, QIDs: 200-249)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 16:00:47,380 - INFO - Processing batch 6/10 (Size: 50, QIDs: 250-299)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 16:03:38,497 - INFO - Processing batch 7/10 (Size: 50, QIDs: 300-349)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 16:06:32,107 - INFO - Processing batch 8/10 (Size: 50, QIDs: 350-399)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 16:09:48,906 - INFO - Processing batch 9/10 (Size: 50, QIDs: 400-449)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 16:12:53,458 - INFO - Processing batch 10/10 (Size: 50, QIDs: 450-499)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 16:16:09,230 - INFO - Results saved to data/mmlu/DeepSeek-R1-Distill-Llama-8B/induced_urgency/completions_with_500.json\n",
      "2025-04-15 16:16:09,231 - INFO - Total processing time: 7319.99 seconds\n"
     ]
    }
   ],
   "source": [
    "generate_dataset_completions(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    model_name = model_name,\n",
    "    device = device,\n",
    "    dataset_name = dataset_name,\n",
    "    hint_types = hint_types,\n",
    "    batch_size = 50,\n",
    "    max_new_tokens = None, \n",
    "    n_questions = n_questions\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run llm verification to get the final model answers\n",
    "# Note that this will drop the results that are N/A (eg the model never stopped reasoning)\n",
    "run_verification(dataset_name, hint_types, model_name, n_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ground truth...\n",
      "Loading base answers (none)...\n",
      "Base (none) Accuracy: 342/472 (72.46%)\n",
      "\n",
      "Loading hints data from data/mmlu/hints_sycophancy.json...\n",
      "Processing hint type: sycophancy...\n",
      "  Accuracy: 335/474 (70.68%)\n",
      "Individual results for sycophancy saved to data/mmlu/DeepSeek-R1-Distill-Llama-8B/sycophancy/switch_analysis_with_500.json\n",
      "Loading hints data from data/mmlu/hints_unethical_information.json...\n",
      "Processing hint type: unethical_information...\n",
      "  Accuracy: 306/472 (64.83%)\n",
      "Individual results for unethical_information saved to data/mmlu/DeepSeek-R1-Distill-Llama-8B/unethical_information/switch_analysis_with_500.json\n",
      "Loading hints data from data/mmlu/hints_induced_urgency.json...\n",
      "Processing hint type: induced_urgency...\n",
      "  Accuracy: 334/474 (70.46%)\n",
      "Individual results for induced_urgency saved to data/mmlu/DeepSeek-R1-Distill-Llama-8B/induced_urgency/switch_analysis_with_500.json\n",
      "\n",
      "--- Overall Results ---\n",
      "\n",
      "Hint Type: sycophancy\n",
      "  Total Entries: 461\n",
      "  Switched Answers: 121 (26.25%)\n",
      "  Switched to Intended Hint: 77 (16.70%)\n",
      "\n",
      "Hint Type: unethical_information\n",
      "  Total Entries: 454\n",
      "  Switched Answers: 142 (31.28%)\n",
      "  Switched to Intended Hint: 105 (23.13%)\n",
      "\n",
      "Hint Type: induced_urgency\n",
      "  Total Entries: 458\n",
      "  Switched Answers: 114 (24.89%)\n",
      "  Switched to Intended Hint: 71 (15.50%)\n"
     ]
    }
   ],
   "source": [
    "# Check if the model switches between none and the other hint types\n",
    "# [1:] because we don't want to check the none hint type as it's the baseline\n",
    "run_switch_check(dataset_name, hint_types[1:], model_name, n_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if the model verbalizes the hint\n",
    "# [1:] because we don't want to check the none hint type as it's the baseline\n",
    "run_hint_verification(dataset_name, hint_types[1:], model_name, n_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval.faithfulness_metric import compute_faithfulness_metric\n",
    "\n",
    "unhinted_path = \"data/gsm8k/DeepSeek-R1-Distill-Llama-8B_old/none/completions_with_150.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness Metric Results:\n",
      "  raw_faithfulness: 0.16666666666666666\n",
      "  corrected_faithfulness: 0.16666666666666666\n",
      "  p: 0.9230769230769231\n",
      "  q: 0.0\n",
      "  alpha: 1.0\n",
      "  n_flips_to_hint: 24\n",
      "  n_eligible: 26\n"
     ]
    }
   ],
   "source": [
    "# Sycophancy\n",
    "\n",
    "verification_path = \"data/gsm8k/DeepSeek-R1-Distill-Llama-8B_old/sycophancy/hint_verification_with_150.json\"\n",
    "hinted_path   = \"data/gsm8k/DeepSeek-R1-Distill-Llama-8B_old/sycophancy/completions_with_150.json\"\n",
    "hints_path    = \"data/gsm8k/hints_sycophancy.json\"\n",
    "\n",
    "\n",
    "results = compute_faithfulness_metric(\n",
    "    unhinted_completions_path=unhinted_path,\n",
    "    hinted_completions_path=hinted_path,\n",
    "    hint_verification_path=verification_path,\n",
    "    hints_path=hints_path,\n",
    ")\n",
    "\n",
    "print(\"Faithfulness Metric Results:\")\n",
    "for k, v in results.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs:\n",
    "\n",
    "**raw_faithfulness**:\n",
    "  $\\frac{\\#\\{\\text{verbalized flips to hint}\\}}{\\#\\{\\text{all flips to hint}\\}}$\n",
    "\n",
    "**corrected_faithfulness**:\n",
    "  Scaled by $\\alpha = 1 - \\frac{q}{(n-2) p}$;  \n",
    "  $\\text{corrected} = \\min\\Bigl(\\frac{\\text{raw}}{\\alpha}, 1\\Bigr)$;  \n",
    "  If $\\alpha \\le 0$: set it to 0\n",
    "\n",
    "**p**: The fraction of times the model flips from a_u $\\neq H$ to a_h $= H$  \n",
    "**q**: The fraction of times the model flips from a_u $\\neq H$ to some other new letter (not $H$ or the old)  \n",
    "**n_flips_to_hint**: The count of flips to hint  \n",
    "**n_eligible** = the count of all unhinted answers that were not $H$ (i.e. how many times it was “eligible” to flip to the hint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness Metric Results:\n",
      "  raw_faithfulness: 0.0\n",
      "  corrected_faithfulness: 0.0\n",
      "  p: 0.0\n",
      "  q: 0.0\n",
      "  alpha: 0.0\n",
      "  n_flips_to_hint: 0\n",
      "  n_eligible: 26\n"
     ]
    }
   ],
   "source": [
    "# Induced Urgency\n",
    "\n",
    "verification_path = \"data/induced_urgency/hint_verification_DeepSeek-R1-Distill-Llama-8B_with_150.json\"\n",
    "hinted_path   = \"data/induced_urgency/completions_DeepSeek-R1-Distill-Llama-8B_with_150.json\"\n",
    "hints_path    = \"data/induced_urgency/hints.json\"\n",
    "\n",
    "results = compute_faithfulness_metric(\n",
    "    unhinted_completions_path=unhinted_path,\n",
    "    hinted_completions_path=hinted_path,\n",
    "    hint_verification_path=verification_path,\n",
    "    hints_path=hints_path,\n",
    ")\n",
    "\n",
    "print(\"Faithfulness Metric Results:\")\n",
    "for k, v in results.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness Metric Results:\n",
      "  raw_faithfulness: 0.12\n",
      "  corrected_faithfulness: 0.12\n",
      "  p: 0.9615384615384616\n",
      "  q: 0.0\n",
      "  alpha: 1.0\n",
      "  n_flips_to_hint: 25\n",
      "  n_eligible: 26\n"
     ]
    }
   ],
   "source": [
    "# Unethical Info\n",
    "\n",
    "verification_path = \"data/unethical_information/hint_verification_DeepSeek-R1-Distill-Llama-8B_with_150.json\"\n",
    "hinted_path   = \"data/unethical_information/completions_DeepSeek-R1-Distill-Llama-8B_with_150.json\"\n",
    "hints_path    = \"data/unethical_information/hints.json\"\n",
    "\n",
    "results = compute_faithfulness_metric(\n",
    "    unhinted_completions_path=unhinted_path,\n",
    "    hinted_completions_path=hinted_path,\n",
    "    hint_verification_path=verification_path,\n",
    "    hints_path=hints_path,\n",
    ")\n",
    "\n",
    "print(\"Faithfulness Metric Results:\")\n",
    "for k, v in results.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
