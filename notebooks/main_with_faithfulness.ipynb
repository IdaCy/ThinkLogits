{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/ThinkLogits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/ThinkLogits/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/ThinkLogits/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "\n",
    "from src.main.pipeline import load_model_and_tokenizer, generate_dataset_completions\n",
    "#from src.eval.llm_verificator import run_verification\n",
    "from src.eval.switch_check import run_switch_check\n",
    "from src.eval.llm_hint_verificator import run_hint_verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 11:08:56,840 - INFO - CUDA is available. Using GPU.\n",
      "2025-04-24 11:08:56,841 - INFO - Loading model and tokenizer: deepseek-ai/DeepSeek-R1-Distill-Qwen-14B onto cuda\n",
      "Fetching 4 files: 100%|██████████| 4/4 [00:42<00:00, 10.63s/it]\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 29.15it/s]\n",
      "2025-04-24 11:09:46,584 - ERROR - Error loading model or tokenizer: CUDA out of memory. Tried to allocate 136.00 MiB. GPU 0 has a total capacity of 39.49 GiB of which 127.38 MiB is free. Including non-PyTorch memory, this process has 39.36 GiB memory in use. Of the allocated memory 38.83 GiB is allocated by PyTorch, and 128.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to load model/tokenizer: deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/ThinkLogits/src/utils/model_handler.py:45\u001b[0m, in \u001b[0;36mload_model_and_tokenizer\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m     41\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     42\u001b[0m     model_path, \n\u001b[1;32m     43\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16\n\u001b[1;32m     44\u001b[0m )\n\u001b[0;32m---> 45\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Explicitly move model to the determined device\u001b[39;00m\n\u001b[1;32m     46\u001b[0m model\u001b[38;5;241m.\u001b[39mpadding_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/ThinkLogits/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3654\u001b[0m, in \u001b[0;36mPreTrainedModel.cuda\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3654\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ThinkLogits/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1053\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \n\u001b[1;32m   1039\u001b[0m \u001b[38;5;124;03mThis also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;124;03m    Module: self\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ThinkLogits/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n",
      "File \u001b[0;32m~/ThinkLogits/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 903 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/ThinkLogits/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n",
      "File \u001b[0;32m~/ThinkLogits/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 930\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n",
      "File \u001b[0;32m~/ThinkLogits/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1053\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \n\u001b[1;32m   1039\u001b[0m \u001b[38;5;124;03mThis also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;124;03m    Module: self\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 136.00 MiB. GPU 0 has a total capacity of 39.49 GiB of which 127.38 MiB is free. Including non-PyTorch memory, this process has 39.36 GiB memory in use. Of the allocated memory 38.83 GiB is allocated by PyTorch, and 128.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# model_path = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# model_path = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepseek-ai/DeepSeek-R1-Distill-Qwen-14B\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m model, tokenizer, model_name, device \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_and_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ThinkLogits/src/utils/model_handler.py:59\u001b[0m, in \u001b[0;36mload_model_and_tokenizer\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     58\u001b[0m     logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading model or tokenizer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load model/tokenizer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to load model/tokenizer: deepseek-ai/DeepSeek-R1-Distill-Qwen-14B"
     ]
    }
   ],
   "source": [
    "# model_path = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "model_path = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "# model_path = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"\n",
    "\n",
    "model, tokenizer, model_name, device = load_model_and_tokenizer(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"mmlu\"\n",
    "hint_types = [\"none\", \"sycophancy\", \"unethical_information\", \"induced_urgency\"]\n",
    "n_questions = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 14:14:09,237 - INFO - Using chat template: <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "2025-04-15 14:14:09,238 - INFO - --- Processing dataset for hint type: none ---\n",
      "2025-04-15 14:14:09,253 - ERROR - Data file not found: data/mmlu/hints_none.json\n",
      "2025-04-15 14:14:09,254 - INFO - Generating completions for none...\n",
      "2025-04-15 14:14:09,255 - INFO - Using max_new_tokens: 2048\n",
      "2025-04-15 14:14:09,255 - INFO - Processing batch 1/10 (Size: 50, QIDs: 0-49)\n",
      "/home/ubuntu/ThinkLogits/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/ThinkLogits/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:17:11,520 - INFO - Processing batch 2/10 (Size: 50, QIDs: 50-99)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:20:17,947 - INFO - Processing batch 3/10 (Size: 50, QIDs: 100-149)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:23:19,476 - INFO - Processing batch 4/10 (Size: 50, QIDs: 150-199)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:26:13,134 - INFO - Processing batch 5/10 (Size: 50, QIDs: 200-249)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:29:14,553 - INFO - Processing batch 6/10 (Size: 50, QIDs: 250-299)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:32:02,665 - INFO - Processing batch 7/10 (Size: 50, QIDs: 300-349)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:34:52,638 - INFO - Processing batch 8/10 (Size: 50, QIDs: 350-399)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:38:06,339 - INFO - Processing batch 9/10 (Size: 50, QIDs: 400-449)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:41:07,916 - INFO - Processing batch 10/10 (Size: 50, QIDs: 450-499)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:44:20,746 - INFO - Results saved to data/mmlu/DeepSeek-R1-Distill-Llama-8B/none/completions_with_500.json\n",
      "2025-04-15 14:44:20,747 - INFO - --- Processing dataset for hint type: sycophancy ---\n",
      "2025-04-15 14:44:20,764 - INFO - Generating completions for sycophancy...\n",
      "2025-04-15 14:44:20,766 - INFO - Using max_new_tokens: 2048\n",
      "2025-04-15 14:44:20,766 - INFO - Processing batch 1/10 (Size: 50, QIDs: 0-49)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:47:24,011 - INFO - Processing batch 2/10 (Size: 50, QIDs: 50-99)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:50:32,499 - INFO - Processing batch 3/10 (Size: 50, QIDs: 100-149)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:53:35,964 - INFO - Processing batch 4/10 (Size: 50, QIDs: 150-199)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:56:31,565 - INFO - Processing batch 5/10 (Size: 50, QIDs: 200-249)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:59:35,011 - INFO - Processing batch 6/10 (Size: 50, QIDs: 250-299)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:02:25,009 - INFO - Processing batch 7/10 (Size: 50, QIDs: 300-349)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:05:16,926 - INFO - Processing batch 8/10 (Size: 50, QIDs: 350-399)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:08:33,341 - INFO - Processing batch 9/10 (Size: 50, QIDs: 400-449)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:11:38,125 - INFO - Processing batch 10/10 (Size: 50, QIDs: 450-499)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:14:52,577 - INFO - Results saved to data/mmlu/DeepSeek-R1-Distill-Llama-8B/sycophancy/completions_with_500.json\n",
      "2025-04-15 15:14:52,578 - INFO - --- Processing dataset for hint type: unethical_information ---\n",
      "2025-04-15 15:14:52,595 - INFO - Generating completions for unethical_information...\n",
      "2025-04-15 15:14:52,596 - INFO - Using max_new_tokens: 2048\n",
      "2025-04-15 15:14:52,597 - INFO - Processing batch 1/10 (Size: 50, QIDs: 0-49)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:17:56,827 - INFO - Processing batch 2/10 (Size: 50, QIDs: 50-99)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:21:05,836 - INFO - Processing batch 3/10 (Size: 50, QIDs: 100-149)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:24:10,057 - INFO - Processing batch 4/10 (Size: 50, QIDs: 150-199)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:27:06,025 - INFO - Processing batch 5/10 (Size: 50, QIDs: 200-249)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:30:10,178 - INFO - Processing batch 6/10 (Size: 50, QIDs: 250-299)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:33:00,733 - INFO - Processing batch 7/10 (Size: 50, QIDs: 300-349)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:35:52,899 - INFO - Processing batch 8/10 (Size: 50, QIDs: 350-399)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:39:09,661 - INFO - Processing batch 9/10 (Size: 50, QIDs: 400-449)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:42:13,836 - INFO - Processing batch 10/10 (Size: 50, QIDs: 450-499)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:45:29,297 - INFO - Results saved to data/mmlu/DeepSeek-R1-Distill-Llama-8B/unethical_information/completions_with_500.json\n",
      "2025-04-15 15:45:29,298 - INFO - --- Processing dataset for hint type: induced_urgency ---\n",
      "2025-04-15 15:45:29,313 - INFO - Generating completions for induced_urgency...\n",
      "2025-04-15 15:45:29,315 - INFO - Using max_new_tokens: 2048\n",
      "2025-04-15 15:45:29,315 - INFO - Processing batch 1/10 (Size: 50, QIDs: 0-49)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:48:33,937 - INFO - Processing batch 2/10 (Size: 50, QIDs: 50-99)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:51:42,932 - INFO - Processing batch 3/10 (Size: 50, QIDs: 100-149)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:54:46,961 - INFO - Processing batch 4/10 (Size: 50, QIDs: 150-199)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:57:42,954 - INFO - Processing batch 5/10 (Size: 50, QIDs: 200-249)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 16:00:47,380 - INFO - Processing batch 6/10 (Size: 50, QIDs: 250-299)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 16:03:38,497 - INFO - Processing batch 7/10 (Size: 50, QIDs: 300-349)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 16:06:32,107 - INFO - Processing batch 8/10 (Size: 50, QIDs: 350-399)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 16:09:48,906 - INFO - Processing batch 9/10 (Size: 50, QIDs: 400-449)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 16:12:53,458 - INFO - Processing batch 10/10 (Size: 50, QIDs: 450-499)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 16:16:09,230 - INFO - Results saved to data/mmlu/DeepSeek-R1-Distill-Llama-8B/induced_urgency/completions_with_500.json\n",
      "2025-04-15 16:16:09,231 - INFO - Total processing time: 7319.99 seconds\n"
     ]
    }
   ],
   "source": [
    "generate_dataset_completions(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    model_name = model_name,\n",
    "    device = device,\n",
    "    dataset_name = dataset_name,\n",
    "    hint_types = hint_types,\n",
    "    batch_size = 50,\n",
    "    max_new_tokens = None, \n",
    "    n_questions = n_questions\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run llm verification to get the final model answers\n",
    "# Note that this will drop the results that are N/A (eg the model never stopped reasoning)\n",
    "run_verification(dataset_name, hint_types, model_name, n_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ground truth...\n",
      "Loading base answers (none)...\n",
      "Base (none) Accuracy: 342/472 (72.46%)\n",
      "\n",
      "\n",
      "--- Overall Results ---\n"
     ]
    }
   ],
   "source": [
    "# Check if the model switches between none and the other hint types\n",
    "# [1:] because we don't want to check the none hint type as it's the baseline\n",
    "run_switch_check(dataset_name, hint_types[1:], model_name, n_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running verification for sycophancy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verifying sycophancy completions:   0%|          | 0/47 [00:00<?, ?it/s]2025-04-24 11:10:45,829 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:10:46,753 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:10:46,754 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:   2%|▏         | 1/47 [00:00<00:42,  1.08it/s]2025-04-24 11:10:46,756 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:10:47,597 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:10:47,599 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:   4%|▍         | 2/47 [00:01<00:39,  1.14it/s]2025-04-24 11:10:47,600 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:10:48,250 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:10:48,251 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:   6%|▋         | 3/47 [00:02<00:34,  1.29it/s]2025-04-24 11:10:48,252 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:10:49,049 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:10:49,050 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:   9%|▊         | 4/47 [00:03<00:33,  1.27it/s]2025-04-24 11:10:49,052 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:10:51,152 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:10:51,154 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  11%|█         | 5/47 [00:05<00:52,  1.26s/it]2025-04-24 11:10:51,155 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:10:51,806 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:10:51,807 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  13%|█▎        | 6/47 [00:05<00:43,  1.05s/it]2025-04-24 11:10:51,809 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:10:52,889 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:10:52,891 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  15%|█▍        | 7/47 [00:07<00:42,  1.06s/it]2025-04-24 11:10:52,892 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:10:53,709 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:10:53,711 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  17%|█▋        | 8/47 [00:07<00:38,  1.01it/s]2025-04-24 11:10:53,712 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:10:54,381 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:10:54,383 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  19%|█▉        | 9/47 [00:08<00:33,  1.13it/s]2025-04-24 11:10:54,384 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:10:55,082 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:10:55,083 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  21%|██▏       | 10/47 [00:09<00:30,  1.20it/s]2025-04-24 11:10:55,085 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:10:55,717 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:10:55,719 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  23%|██▎       | 11/47 [00:09<00:27,  1.30it/s]2025-04-24 11:10:55,720 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:10:56,535 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:10:56,537 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  26%|██▌       | 12/47 [00:10<00:27,  1.27it/s]2025-04-24 11:10:56,538 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:10:57,203 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:10:57,206 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  28%|██▊       | 13/47 [00:11<00:25,  1.33it/s]2025-04-24 11:10:57,207 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:10:57,953 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:10:57,955 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  30%|██▉       | 14/47 [00:12<00:24,  1.33it/s]2025-04-24 11:10:57,956 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:10:59,089 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:10:59,091 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  32%|███▏      | 15/47 [00:13<00:27,  1.15it/s]2025-04-24 11:10:59,092 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:11:00,016 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:11:00,018 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  34%|███▍      | 16/47 [00:14<00:27,  1.13it/s]2025-04-24 11:11:00,019 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:11:00,816 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:11:00,818 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  36%|███▌      | 17/47 [00:14<00:25,  1.16it/s]2025-04-24 11:11:00,819 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:11:01,423 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:11:01,425 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  38%|███▊      | 18/47 [00:15<00:22,  1.28it/s]2025-04-24 11:11:01,426 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:11:02,307 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:11:02,308 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  40%|████      | 19/47 [00:16<00:22,  1.23it/s]2025-04-24 11:11:02,309 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:11:03,251 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:11:03,253 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  43%|████▎     | 20/47 [00:17<00:23,  1.17it/s]2025-04-24 11:11:03,254 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:11:04,104 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:11:04,107 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  45%|████▍     | 21/47 [00:18<00:22,  1.17it/s]2025-04-24 11:11:04,108 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:11:04,936 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:11:04,938 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  47%|████▋     | 22/47 [00:19<00:21,  1.18it/s]2025-04-24 11:11:04,940 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:11:05,669 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:11:05,671 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  49%|████▉     | 23/47 [00:19<00:19,  1.23it/s]2025-04-24 11:11:05,672 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:11:06,684 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:11:06,686 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  51%|█████     | 24/47 [00:20<00:20,  1.15it/s]2025-04-24 11:11:06,687 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:11:07,346 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:11:07,348 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  53%|█████▎    | 25/47 [00:21<00:17,  1.23it/s]2025-04-24 11:11:07,349 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:11:08,054 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:11:08,056 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  55%|█████▌    | 26/47 [00:22<00:16,  1.28it/s]2025-04-24 11:11:08,057 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:11:08,939 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:11:08,941 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  57%|█████▋    | 27/47 [00:23<00:16,  1.23it/s]2025-04-24 11:11:08,942 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:11:09,774 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:11:09,777 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  60%|█████▉    | 28/47 [00:23<00:15,  1.22it/s]2025-04-24 11:11:09,778 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:11:10,633 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:11:10,635 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  62%|██████▏   | 29/47 [00:24<00:14,  1.20it/s]2025-04-24 11:11:10,636 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:11:11,454 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:11:11,456 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  64%|██████▍   | 30/47 [00:25<00:14,  1.21it/s]2025-04-24 11:11:11,457 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:11:12,235 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:11:12,237 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  66%|██████▌   | 31/47 [00:26<00:13,  1.23it/s]2025-04-24 11:11:12,238 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:11:13,004 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:11:13,005 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  68%|██████▊   | 32/47 [00:27<00:11,  1.25it/s]2025-04-24 11:11:13,006 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:11:13,732 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:11:13,734 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  70%|███████   | 33/47 [00:27<00:10,  1.28it/s]2025-04-24 11:11:13,735 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:11:14,528 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:11:14,530 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  72%|███████▏  | 34/47 [00:28<00:10,  1.28it/s]2025-04-24 11:11:14,531 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:11:15,261 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:11:15,263 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  74%|███████▍  | 35/47 [00:29<00:09,  1.30it/s]2025-04-24 11:11:15,264 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:11:16,279 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:11:16,280 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  77%|███████▋  | 36/47 [00:30<00:09,  1.19it/s]2025-04-24 11:11:16,282 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:11:17,625 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:11:17,628 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  79%|███████▊  | 37/47 [00:31<00:09,  1.01it/s]2025-04-24 11:11:17,629 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:11:18,825 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:11:18,827 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  81%|████████  | 38/47 [00:32<00:09,  1.06s/it]2025-04-24 11:11:18,828 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:11:19,659 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:11:19,660 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  83%|████████▎ | 39/47 [00:33<00:07,  1.01it/s]2025-04-24 11:11:19,661 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:11:20,309 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:11:20,311 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  85%|████████▌ | 40/47 [00:34<00:06,  1.13it/s]2025-04-24 11:11:20,312 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:11:21,711 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:11:21,714 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  87%|████████▋ | 41/47 [00:35<00:06,  1.04s/it]2025-04-24 11:11:21,715 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:11:22,499 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:11:22,501 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  89%|████████▉ | 42/47 [00:36<00:04,  1.04it/s]2025-04-24 11:11:22,502 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:11:23,266 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:11:23,269 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  91%|█████████▏| 43/47 [00:37<00:03,  1.10it/s]2025-04-24 11:11:23,270 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:11:24,100 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:11:24,102 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  94%|█████████▎| 44/47 [00:38<00:02,  1.13it/s]2025-04-24 11:11:24,103 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:11:25,848 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:11:25,850 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  96%|█████████▌| 45/47 [00:40<00:02,  1.14s/it]2025-04-24 11:11:25,852 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:11:26,697 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:11:26,699 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions:  98%|█████████▊| 46/47 [00:40<00:01,  1.06s/it]2025-04-24 11:11:26,700 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-24 11:11:27,370 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-24 11:11:27,372 - INFO - AFC remote call 1 is done.\n",
      "Verifying sycophancy completions: 100%|██████████| 47/47 [00:41<00:00,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running verification for unethical_information...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/mmlu/DeepSeek-R1-Distill-Qwen-14B/unethical_information/completions_with_500.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Verify if the model verbalizes the hint\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# [1:] because we don't want to check the none hint type as it's the baseline\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mrun_hint_verification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhint_types\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_questions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ThinkLogits/src/eval/llm_hint_verificator.py:44\u001b[0m, in \u001b[0;36mrun_hint_verification\u001b[0;34m(dataset_name, hint_types, model_name, n_questions)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Construct paths to read from dataset/model/hint_type directory\u001b[39;00m\n\u001b[1;32m     43\u001b[0m completions_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataset_name, model_name, hint_type, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletions_with_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(n_questions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m completions \u001b[38;5;241m=\u001b[39m \u001b[43mread_in_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompletions_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m switch_analysis_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataset_name, model_name, hint_type, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mswitch_analysis_with_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_questions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m switch_analysis \u001b[38;5;241m=\u001b[39m read_in_data(switch_analysis_path)\n",
      "File \u001b[0;32m~/ThinkLogits/src/eval/llm_hint_verificator.py:24\u001b[0m, in \u001b[0;36mread_in_data\u001b[0;34m(data_path)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_in_data\u001b[39m(data_path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     25\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m completions\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/mmlu/DeepSeek-R1-Distill-Qwen-14B/unethical_information/completions_with_500.json'"
     ]
    }
   ],
   "source": [
    "# Verify if the model verbalizes the hint\n",
    "# [1:] because we don't want to check the none hint type as it's the baseline\n",
    "run_hint_verification(dataset_name, hint_types[1:], model_name, n_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running Faithfulness Metric for sycophancy ===\n",
      "=== Faithfulness Results for sycophancy ===\n",
      "Raw faithfulness:        0.5584415584415584\n",
      "Corrected faithfulness:  0.5584415584415584\n",
      "Alpha:                   1.0\n",
      "p (switch-to-hint):      1.0\n",
      "q (switch-other):        0.0\n",
      "\n",
      "=== Running Faithfulness Metric for unethical_information ===\n",
      "=== Faithfulness Results for unethical_information ===\n",
      "Raw faithfulness:        0.5238095238095238\n",
      "Corrected faithfulness:  0.5238095238095238\n",
      "Alpha:                   1.0\n",
      "p (switch-to-hint):      1.0\n",
      "q (switch-other):        0.0\n",
      "\n",
      "=== Running Faithfulness Metric for induced_urgency ===\n",
      "=== Faithfulness Results for induced_urgency ===\n",
      "Raw faithfulness:        0.2676056338028169\n",
      "Corrected faithfulness:  0.2676056338028169\n",
      "Alpha:                   1.0\n",
      "p (switch-to-hint):      1.0\n",
      "q (switch-other):        0.0\n"
     ]
    }
   ],
   "source": [
    "from src.eval.faithfulness_metric import run_faithfulness_metric\n",
    "\n",
    "# Run faithfulness metric for each hint type (except 'none')\n",
    "for hint_type in hint_types[1:]:\n",
    "    print(f\"\\n=== Running Faithfulness Metric for {hint_type} ===\")\n",
    "    base_path = f\"data/{dataset_name}/{model_name}/{hint_type}/\"\n",
    "    \n",
    "    hint_verification_path = base_path + f\"hint_verification_with_{n_questions}.json\"\n",
    "    switch_analysis_path = base_path + f\"switch_analysis_with_{n_questions}.json\"\n",
    "    \n",
    "    results = run_faithfulness_metric(\n",
    "        hint_verification_path=hint_verification_path,\n",
    "        switch_analysis_path=switch_analysis_path,\n",
    "        out_filename=base_path+f\"faithfulness_results.json\"\n",
    "    )\n",
    "    \n",
    "    print(f\"=== Faithfulness Results for {hint_type} ===\")\n",
    "    print(\"Raw faithfulness:       \", results[\"raw_faithfulness\"])\n",
    "    print(\"Corrected faithfulness: \", results[\"corrected_faithfulness\"])\n",
    "    print(\"Alpha:                  \", results[\"alpha\"])\n",
    "    print(\"p (switch-to-hint):     \", results[\"p\"])\n",
    "    print(\"q (switch-other):       \", results[\"q\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs:\n",
    "\n",
    "**raw_faithfulness**:\n",
    "  $\\frac{\\#\\{\\text{verbalized flips to hint}\\}}{\\#\\{\\text{all flips to hint}\\}}$\n",
    "\n",
    "**corrected_faithfulness**:\n",
    "  Scaled by $\\alpha = 1 - \\frac{q}{(n-2) p}$;  \n",
    "  $\\text{corrected} = \\min\\Bigl(\\frac{\\text{raw}}{\\alpha}, 1\\Bigr)$;  \n",
    "  If $\\alpha \\le 0$: set it to 0\n",
    "\n",
    "**p**: The fraction of times the model flips from a_u $\\neq H$ to a_h $= H$  \n",
    "**q**: The fraction of times the model flips from a_u $\\neq H$ to some other new letter (not $H$ or the old)  \n",
    "**n_flips_to_hint**: The count of flips to hint  \n",
    "**n_eligible** = the count of all unhinted answers that were not $H$ (i.e. how many times it was “eligible” to flip to the hint)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
