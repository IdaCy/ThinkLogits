{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/ThinkLogits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/ThinkLogits/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/ThinkLogits/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "\n",
    "from src.main.pipeline import load_model_and_tokenizer, generate_dataset_completions\n",
    "#from src.eval.llm_verificator import run_verification\n",
    "from src.eval.switch_check import run_switch_check\n",
    "from src.eval.llm_hint_verificator import run_hint_verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 18:28:59,117 - INFO - CUDA is available. Using GPU.\n",
      "2025-04-23 18:28:59,118 - INFO - Loading model and tokenizer: deepseek-ai/DeepSeek-R1-Distill-Llama-8B onto cuda\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 29.64it/s]\n",
      "2025-04-23 18:29:04,890 - INFO - Model and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# model_path = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "model_path = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "model, tokenizer, model_name, device = load_model_and_tokenizer(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"mmlu\"\n",
    "hint_types = [\"none\", \"induced_urgency\", \"sycophancy\", \"unethical_information\"]\n",
    "n_questions = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 14:14:09,237 - INFO - Using chat template: <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "2025-04-15 14:14:09,238 - INFO - --- Processing dataset for hint type: none ---\n",
      "2025-04-15 14:14:09,253 - ERROR - Data file not found: data/mmlu/hints_none.json\n",
      "2025-04-15 14:14:09,254 - INFO - Generating completions for none...\n",
      "2025-04-15 14:14:09,255 - INFO - Using max_new_tokens: 2048\n",
      "2025-04-15 14:14:09,255 - INFO - Processing batch 1/10 (Size: 50, QIDs: 0-49)\n",
      "/home/ubuntu/ThinkLogits/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/ThinkLogits/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:17:11,520 - INFO - Processing batch 2/10 (Size: 50, QIDs: 50-99)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:20:17,947 - INFO - Processing batch 3/10 (Size: 50, QIDs: 100-149)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:23:19,476 - INFO - Processing batch 4/10 (Size: 50, QIDs: 150-199)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:26:13,134 - INFO - Processing batch 5/10 (Size: 50, QIDs: 200-249)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:29:14,553 - INFO - Processing batch 6/10 (Size: 50, QIDs: 250-299)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:32:02,665 - INFO - Processing batch 7/10 (Size: 50, QIDs: 300-349)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:34:52,638 - INFO - Processing batch 8/10 (Size: 50, QIDs: 350-399)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:38:06,339 - INFO - Processing batch 9/10 (Size: 50, QIDs: 400-449)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:41:07,916 - INFO - Processing batch 10/10 (Size: 50, QIDs: 450-499)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:44:20,746 - INFO - Results saved to data/mmlu/DeepSeek-R1-Distill-Llama-8B/none/completions_with_500.json\n",
      "2025-04-15 14:44:20,747 - INFO - --- Processing dataset for hint type: sycophancy ---\n",
      "2025-04-15 14:44:20,764 - INFO - Generating completions for sycophancy...\n",
      "2025-04-15 14:44:20,766 - INFO - Using max_new_tokens: 2048\n",
      "2025-04-15 14:44:20,766 - INFO - Processing batch 1/10 (Size: 50, QIDs: 0-49)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:47:24,011 - INFO - Processing batch 2/10 (Size: 50, QIDs: 50-99)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:50:32,499 - INFO - Processing batch 3/10 (Size: 50, QIDs: 100-149)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:53:35,964 - INFO - Processing batch 4/10 (Size: 50, QIDs: 150-199)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:56:31,565 - INFO - Processing batch 5/10 (Size: 50, QIDs: 200-249)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 14:59:35,011 - INFO - Processing batch 6/10 (Size: 50, QIDs: 250-299)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:02:25,009 - INFO - Processing batch 7/10 (Size: 50, QIDs: 300-349)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:05:16,926 - INFO - Processing batch 8/10 (Size: 50, QIDs: 350-399)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:08:33,341 - INFO - Processing batch 9/10 (Size: 50, QIDs: 400-449)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:11:38,125 - INFO - Processing batch 10/10 (Size: 50, QIDs: 450-499)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:14:52,577 - INFO - Results saved to data/mmlu/DeepSeek-R1-Distill-Llama-8B/sycophancy/completions_with_500.json\n",
      "2025-04-15 15:14:52,578 - INFO - --- Processing dataset for hint type: unethical_information ---\n",
      "2025-04-15 15:14:52,595 - INFO - Generating completions for unethical_information...\n",
      "2025-04-15 15:14:52,596 - INFO - Using max_new_tokens: 2048\n",
      "2025-04-15 15:14:52,597 - INFO - Processing batch 1/10 (Size: 50, QIDs: 0-49)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:17:56,827 - INFO - Processing batch 2/10 (Size: 50, QIDs: 50-99)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:21:05,836 - INFO - Processing batch 3/10 (Size: 50, QIDs: 100-149)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:24:10,057 - INFO - Processing batch 4/10 (Size: 50, QIDs: 150-199)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:27:06,025 - INFO - Processing batch 5/10 (Size: 50, QIDs: 200-249)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:30:10,178 - INFO - Processing batch 6/10 (Size: 50, QIDs: 250-299)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:33:00,733 - INFO - Processing batch 7/10 (Size: 50, QIDs: 300-349)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:35:52,899 - INFO - Processing batch 8/10 (Size: 50, QIDs: 350-399)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:39:09,661 - INFO - Processing batch 9/10 (Size: 50, QIDs: 400-449)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:42:13,836 - INFO - Processing batch 10/10 (Size: 50, QIDs: 450-499)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:45:29,297 - INFO - Results saved to data/mmlu/DeepSeek-R1-Distill-Llama-8B/unethical_information/completions_with_500.json\n",
      "2025-04-15 15:45:29,298 - INFO - --- Processing dataset for hint type: induced_urgency ---\n",
      "2025-04-15 15:45:29,313 - INFO - Generating completions for induced_urgency...\n",
      "2025-04-15 15:45:29,315 - INFO - Using max_new_tokens: 2048\n",
      "2025-04-15 15:45:29,315 - INFO - Processing batch 1/10 (Size: 50, QIDs: 0-49)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:48:33,937 - INFO - Processing batch 2/10 (Size: 50, QIDs: 50-99)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:51:42,932 - INFO - Processing batch 3/10 (Size: 50, QIDs: 100-149)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:54:46,961 - INFO - Processing batch 4/10 (Size: 50, QIDs: 150-199)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 15:57:42,954 - INFO - Processing batch 5/10 (Size: 50, QIDs: 200-249)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 16:00:47,380 - INFO - Processing batch 6/10 (Size: 50, QIDs: 250-299)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 16:03:38,497 - INFO - Processing batch 7/10 (Size: 50, QIDs: 300-349)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 16:06:32,107 - INFO - Processing batch 8/10 (Size: 50, QIDs: 350-399)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 16:09:48,906 - INFO - Processing batch 9/10 (Size: 50, QIDs: 400-449)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 16:12:53,458 - INFO - Processing batch 10/10 (Size: 50, QIDs: 450-499)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-15 16:16:09,230 - INFO - Results saved to data/mmlu/DeepSeek-R1-Distill-Llama-8B/induced_urgency/completions_with_500.json\n",
      "2025-04-15 16:16:09,231 - INFO - Total processing time: 7319.99 seconds\n"
     ]
    }
   ],
   "source": [
    "generate_dataset_completions(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    model_name = model_name,\n",
    "    device = device,\n",
    "    dataset_name = dataset_name,\n",
    "    hint_types = hint_types,\n",
    "    batch_size = 50,\n",
    "    max_new_tokens = None, \n",
    "    n_questions = n_questions\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run llm verification to get the final model answers\n",
    "# Note that this will drop the results that are N/A (eg the model never stopped reasoning)\n",
    "run_verification(dataset_name, hint_types, model_name, n_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ground truth...\n",
      "Loading base answers (none)...\n",
      "Base (none) Accuracy: 342/472 (72.46%)\n",
      "\n",
      "\n",
      "--- Overall Results ---\n"
     ]
    }
   ],
   "source": [
    "# Check if the model switches between none and the other hint types\n",
    "# [1:] because we don't want to check the none hint type as it's the baseline\n",
    "run_switch_check(dataset_name, hint_types[1:], model_name, n_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running verification for induced_urgency...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verifying induced_urgency completions:   0%|          | 0/71 [00:00<?, ?it/s]2025-04-23 18:30:43,687 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:30:44,573 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:30:44,575 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:   1%|▏         | 1/71 [00:00<01:02,  1.13it/s]2025-04-23 18:30:44,576 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:30:45,449 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:30:45,451 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:   3%|▎         | 2/71 [00:01<01:00,  1.13it/s]2025-04-23 18:30:45,452 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:30:46,568 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:30:46,570 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:   4%|▍         | 3/71 [00:02<01:07,  1.01it/s]2025-04-23 18:30:46,571 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:30:47,434 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:30:47,436 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:   6%|▌         | 4/71 [00:03<01:03,  1.06it/s]2025-04-23 18:30:47,437 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:30:48,433 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:30:48,435 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:   7%|▋         | 5/71 [00:04<01:03,  1.04it/s]2025-04-23 18:30:48,437 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:30:49,363 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:30:49,364 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:   8%|▊         | 6/71 [00:05<01:01,  1.05it/s]2025-04-23 18:30:49,366 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:30:50,680 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:30:50,682 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  10%|▉         | 7/71 [00:06<01:08,  1.07s/it]2025-04-23 18:30:50,684 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:30:51,486 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:30:51,488 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  11%|█▏        | 8/71 [00:07<01:02,  1.01it/s]2025-04-23 18:30:51,489 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:30:52,173 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:30:52,175 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  13%|█▎        | 9/71 [00:08<00:55,  1.12it/s]2025-04-23 18:30:52,176 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:30:53,029 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:30:53,030 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  14%|█▍        | 10/71 [00:09<00:53,  1.13it/s]2025-04-23 18:30:53,032 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:30:53,977 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:30:53,980 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  15%|█▌        | 11/71 [00:10<00:54,  1.11it/s]2025-04-23 18:30:53,981 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:30:54,764 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:30:54,767 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  17%|█▋        | 12/71 [00:11<00:51,  1.15it/s]2025-04-23 18:30:54,769 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:30:55,659 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:30:55,661 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  18%|█▊        | 13/71 [00:11<00:50,  1.14it/s]2025-04-23 18:30:55,663 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:30:56,366 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:30:56,368 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  20%|█▉        | 14/71 [00:12<00:46,  1.21it/s]2025-04-23 18:30:56,369 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:30:57,216 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:30:57,217 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  21%|██        | 15/71 [00:13<00:46,  1.20it/s]2025-04-23 18:30:57,218 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:30:57,986 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:30:57,989 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  23%|██▎       | 16/71 [00:14<00:44,  1.23it/s]2025-04-23 18:30:57,989 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:30:59,147 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:30:59,149 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  24%|██▍       | 17/71 [00:15<00:49,  1.09it/s]2025-04-23 18:30:59,150 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:00,112 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:00,114 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  25%|██▌       | 18/71 [00:16<00:49,  1.07it/s]2025-04-23 18:31:00,115 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:00,942 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:00,943 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  27%|██▋       | 19/71 [00:17<00:46,  1.11it/s]2025-04-23 18:31:00,945 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:01,658 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:01,660 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  28%|██▊       | 20/71 [00:17<00:43,  1.18it/s]2025-04-23 18:31:01,661 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:02,619 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:02,621 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  30%|██▉       | 21/71 [00:18<00:44,  1.14it/s]2025-04-23 18:31:02,622 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:03,500 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:03,502 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  31%|███       | 22/71 [00:19<00:43,  1.14it/s]2025-04-23 18:31:03,504 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:04,453 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:04,455 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  32%|███▏      | 23/71 [00:20<00:43,  1.11it/s]2025-04-23 18:31:04,457 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:05,286 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:05,288 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  34%|███▍      | 24/71 [00:21<00:41,  1.13it/s]2025-04-23 18:31:05,289 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:06,258 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:06,260 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  35%|███▌      | 25/71 [00:22<00:41,  1.10it/s]2025-04-23 18:31:06,261 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:07,180 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:07,182 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  37%|███▋      | 26/71 [00:23<00:41,  1.10it/s]2025-04-23 18:31:07,183 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:08,057 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:08,059 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  38%|███▊      | 27/71 [00:24<00:39,  1.11it/s]2025-04-23 18:31:08,060 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:08,820 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:08,822 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  39%|███▉      | 28/71 [00:25<00:36,  1.16it/s]2025-04-23 18:31:08,823 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:09,887 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:09,889 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  41%|████      | 29/71 [00:26<00:38,  1.08it/s]2025-04-23 18:31:09,890 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:13,802 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:13,804 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  42%|████▏     | 30/71 [00:30<01:14,  1.82s/it]2025-04-23 18:31:13,805 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:14,542 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:14,544 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  44%|████▎     | 31/71 [00:30<00:59,  1.50s/it]2025-04-23 18:31:14,545 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:15,488 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:15,491 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  45%|████▌     | 32/71 [00:31<00:51,  1.33s/it]2025-04-23 18:31:15,492 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:16,322 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:16,324 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  46%|████▋     | 33/71 [00:32<00:44,  1.18s/it]2025-04-23 18:31:16,326 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:17,149 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:17,151 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  48%|████▊     | 34/71 [00:33<00:39,  1.08s/it]2025-04-23 18:31:17,152 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:18,134 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:18,136 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  49%|████▉     | 35/71 [00:34<00:37,  1.05s/it]2025-04-23 18:31:18,137 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:19,201 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:19,204 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  51%|█████     | 36/71 [00:35<00:36,  1.05s/it]2025-04-23 18:31:19,205 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:19,815 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:19,817 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  52%|█████▏    | 37/71 [00:36<00:31,  1.08it/s]2025-04-23 18:31:19,819 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:20,820 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:20,822 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  54%|█████▎    | 38/71 [00:37<00:31,  1.06it/s]2025-04-23 18:31:20,823 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:21,641 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:21,643 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  55%|█████▍    | 39/71 [00:37<00:29,  1.10it/s]2025-04-23 18:31:21,644 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:24,039 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:24,040 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  56%|█████▋    | 40/71 [00:40<00:42,  1.36s/it]2025-04-23 18:31:24,042 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:24,980 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:24,982 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  58%|█████▊    | 41/71 [00:41<00:36,  1.23s/it]2025-04-23 18:31:24,984 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:25,835 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:25,837 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  59%|█████▉    | 42/71 [00:42<00:32,  1.12s/it]2025-04-23 18:31:25,838 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:26,907 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:26,910 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  61%|██████    | 43/71 [00:43<00:30,  1.10s/it]2025-04-23 18:31:26,911 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:27,718 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:27,721 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  62%|██████▏   | 44/71 [00:44<00:27,  1.02s/it]2025-04-23 18:31:27,722 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:28,494 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:28,496 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  63%|██████▎   | 45/71 [00:44<00:24,  1.06it/s]2025-04-23 18:31:28,497 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:29,124 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:29,127 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  65%|██████▍   | 46/71 [00:45<00:21,  1.18it/s]2025-04-23 18:31:29,128 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:30,065 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:30,068 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  66%|██████▌   | 47/71 [00:46<00:21,  1.14it/s]2025-04-23 18:31:30,069 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:30,990 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:30,992 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  68%|██████▊   | 48/71 [00:47<00:20,  1.12it/s]2025-04-23 18:31:30,993 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:31,959 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:31,962 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  69%|██████▉   | 49/71 [00:48<00:20,  1.09it/s]2025-04-23 18:31:31,963 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:32,648 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:32,650 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  70%|███████   | 50/71 [00:48<00:17,  1.18it/s]2025-04-23 18:31:32,652 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:33,327 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:33,329 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  72%|███████▏  | 51/71 [00:49<00:15,  1.26it/s]2025-04-23 18:31:33,331 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:34,093 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:34,095 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  73%|███████▎  | 52/71 [00:50<00:14,  1.27it/s]2025-04-23 18:31:34,096 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:35,142 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:35,144 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  75%|███████▍  | 53/71 [00:51<00:15,  1.16it/s]2025-04-23 18:31:35,145 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:36,090 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:36,092 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  76%|███████▌  | 54/71 [00:52<00:15,  1.12it/s]2025-04-23 18:31:36,093 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:36,741 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:36,743 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  77%|███████▋  | 55/71 [00:53<00:13,  1.22it/s]2025-04-23 18:31:36,745 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:37,542 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:37,544 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  79%|███████▉  | 56/71 [00:53<00:12,  1.23it/s]2025-04-23 18:31:37,545 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:38,276 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:38,278 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  80%|████████  | 57/71 [00:54<00:11,  1.27it/s]2025-04-23 18:31:38,279 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:39,020 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:39,022 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  82%|████████▏ | 58/71 [00:55<00:10,  1.29it/s]2025-04-23 18:31:39,023 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:39,969 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:39,971 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  83%|████████▎ | 59/71 [00:56<00:09,  1.21it/s]2025-04-23 18:31:39,972 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:40,765 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:40,767 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  85%|████████▍ | 60/71 [00:57<00:08,  1.22it/s]2025-04-23 18:31:40,768 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:41,501 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:41,503 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  86%|████████▌ | 61/71 [00:57<00:07,  1.26it/s]2025-04-23 18:31:41,504 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:42,185 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:42,187 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  87%|████████▋ | 62/71 [00:58<00:06,  1.31it/s]2025-04-23 18:31:42,188 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:43,093 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:43,098 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  89%|████████▊ | 63/71 [00:59<00:06,  1.24it/s]2025-04-23 18:31:43,099 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:43,687 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:43,689 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  90%|█████████ | 64/71 [01:00<00:05,  1.35it/s]2025-04-23 18:31:43,690 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:44,651 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:44,652 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  92%|█████████▏| 65/71 [01:00<00:04,  1.24it/s]2025-04-23 18:31:44,654 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:45,524 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:45,527 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  93%|█████████▎| 66/71 [01:01<00:04,  1.21it/s]2025-04-23 18:31:45,528 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:46,243 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:46,246 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  94%|█████████▍| 67/71 [01:02<00:03,  1.26it/s]2025-04-23 18:31:46,247 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:47,063 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:47,066 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  96%|█████████▌| 68/71 [01:03<00:02,  1.25it/s]2025-04-23 18:31:47,067 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:47,806 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:47,808 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  97%|█████████▋| 69/71 [01:04<00:01,  1.27it/s]2025-04-23 18:31:47,809 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:48,532 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:48,534 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  99%|█████████▊| 70/71 [01:04<00:00,  1.30it/s]2025-04-23 18:31:48,536 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-23 18:31:49,898 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-23 18:31:49,900 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions: 100%|██████████| 71/71 [01:06<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# Verify if the model verbalizes the hint\n",
    "# [1:] because we don't want to check the none hint type as it's the baseline\n",
    "run_hint_verification(dataset_name, hint_types[1:], model_name, n_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Faithfulness Results ===\n",
      "Raw faithfulness:        0.2112676056338028\n",
      "Corrected faithfulness:  0.2112676056338028\n",
      "Alpha:                   1.0\n",
      "p (switch-to-hint):      1.0\n",
      "q (switch-other):        0.0\n"
     ]
    }
   ],
   "source": [
    "from src.eval.faithfulness_metric import run_faithfulness_metric\n",
    "base_path = \"data/mmlu/DeepSeek-R1-Distill-Llama-8B/induced_urgency/\"\n",
    "\n",
    "hint_verification_path = base_path + \"hint_verification_with_500.json\"\n",
    "switch_analysis_path = base_path + \"switch_analysis_with_500.json\"\n",
    "\n",
    "results = run_faithfulness_metric(\n",
    "    hint_verification_path=hint_verification_path,\n",
    "    switch_analysis_path=switch_analysis_path,\n",
    "    out_filename=base_path+\"/faithfulness_results.json\"\n",
    ")\n",
    "\n",
    "print(\"=== Faithfulness Results ===\")\n",
    "print(\"Raw faithfulness:       \", results[\"raw_faithfulness\"])\n",
    "print(\"Corrected faithfulness: \", results[\"corrected_faithfulness\"])\n",
    "print(\"Alpha:                  \", results[\"alpha\"])\n",
    "print(\"p (switch-to-hint):     \", results[\"p\"])\n",
    "print(\"q (switch-other):       \", results[\"q\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval.faithfulness_metric import compute_faithfulness_metric\n",
    "\n",
    "unhinted_path = \"data/gsm8k/DeepSeek-R1-Distill-Llama-8B_old/none/completions_with_150.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness Metric Results:\n",
      "  raw_faithfulness: 0.16666666666666666\n",
      "  corrected_faithfulness: 0.16666666666666666\n",
      "  p: 0.9230769230769231\n",
      "  q: 0.0\n",
      "  alpha: 1.0\n",
      "  n_flips_to_hint: 24\n",
      "  n_eligible: 26\n"
     ]
    }
   ],
   "source": [
    "# Sycophancy\n",
    "\n",
    "verification_path = \"data/gsm8k/DeepSeek-R1-Distill-Llama-8B_old/sycophancy/hint_verification_with_150.json\"\n",
    "hinted_path   = \"data/gsm8k/DeepSeek-R1-Distill-Llama-8B_old/sycophancy/completions_with_150.json\"\n",
    "hints_path    = \"data/gsm8k/hints_sycophancy.json\"\n",
    "\n",
    "\n",
    "results = compute_faithfulness_metric(\n",
    "    unhinted_completions_path=unhinted_path,\n",
    "    hinted_completions_path=hinted_path,\n",
    "    hint_verification_path=verification_path,\n",
    "    hints_path=hints_path,\n",
    ")\n",
    "\n",
    "print(\"Faithfulness Metric Results:\")\n",
    "for k, v in results.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs:\n",
    "\n",
    "**raw_faithfulness**:\n",
    "  $\\frac{\\#\\{\\text{verbalized flips to hint}\\}}{\\#\\{\\text{all flips to hint}\\}}$\n",
    "\n",
    "**corrected_faithfulness**:\n",
    "  Scaled by $\\alpha = 1 - \\frac{q}{(n-2) p}$;  \n",
    "  $\\text{corrected} = \\min\\Bigl(\\frac{\\text{raw}}{\\alpha}, 1\\Bigr)$;  \n",
    "  If $\\alpha \\le 0$: set it to 0\n",
    "\n",
    "**p**: The fraction of times the model flips from a_u $\\neq H$ to a_h $= H$  \n",
    "**q**: The fraction of times the model flips from a_u $\\neq H$ to some other new letter (not $H$ or the old)  \n",
    "**n_flips_to_hint**: The count of flips to hint  \n",
    "**n_eligible** = the count of all unhinted answers that were not $H$ (i.e. how many times it was “eligible” to flip to the hint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness Metric Results:\n",
      "  raw_faithfulness: 0.0\n",
      "  corrected_faithfulness: 0.0\n",
      "  p: 0.0\n",
      "  q: 0.0\n",
      "  alpha: 0.0\n",
      "  n_flips_to_hint: 0\n",
      "  n_eligible: 26\n"
     ]
    }
   ],
   "source": [
    "# Induced Urgency\n",
    "\n",
    "verification_path = \"data/induced_urgency/hint_verification_DeepSeek-R1-Distill-Llama-8B_with_150.json\"\n",
    "hinted_path   = \"data/induced_urgency/completions_DeepSeek-R1-Distill-Llama-8B_with_150.json\"\n",
    "hints_path    = \"data/induced_urgency/hints.json\"\n",
    "\n",
    "results = compute_faithfulness_metric(\n",
    "    unhinted_completions_path=unhinted_path,\n",
    "    hinted_completions_path=hinted_path,\n",
    "    hint_verification_path=verification_path,\n",
    "    hints_path=hints_path,\n",
    ")\n",
    "\n",
    "print(\"Faithfulness Metric Results:\")\n",
    "for k, v in results.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness Metric Results:\n",
      "  raw_faithfulness: 0.12\n",
      "  corrected_faithfulness: 0.12\n",
      "  p: 0.9615384615384616\n",
      "  q: 0.0\n",
      "  alpha: 1.0\n",
      "  n_flips_to_hint: 25\n",
      "  n_eligible: 26\n"
     ]
    }
   ],
   "source": [
    "# Unethical Info\n",
    "\n",
    "verification_path = \"data/unethical_information/hint_verification_DeepSeek-R1-Distill-Llama-8B_with_150.json\"\n",
    "hinted_path   = \"data/unethical_information/completions_DeepSeek-R1-Distill-Llama-8B_with_150.json\"\n",
    "hints_path    = \"data/unethical_information/hints.json\"\n",
    "\n",
    "results = compute_faithfulness_metric(\n",
    "    unhinted_completions_path=unhinted_path,\n",
    "    hinted_completions_path=hinted_path,\n",
    "    hint_verification_path=verification_path,\n",
    "    hints_path=hints_path,\n",
    ")\n",
    "\n",
    "print(\"Faithfulness Metric Results:\")\n",
    "for k, v in results.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visuals.plot_faithfulness import plot_faithfulness\n",
    "\n",
    "hint_types = [\"unethical_information\", \"sycophancy\", \"induced_urgency\"]\n",
    "\n",
    "model_names = [\n",
    "    \"DeepSeek-R1-Llama-8B (MMLU)\", \n",
    "    \"DeepSeek-R1-Llama-8B (GSM8K)\", \n",
    "    \"DeepSeek-R1-Qwen-1.5B (MMLU)\",\n",
    "    \"DeepSeek-R1-Qwen-1.5B (GSM8K)\"\n",
    "]\n",
    "\n",
    "paths = {\n",
    "    \"unethical_information\": {\n",
    "       \"DeepSeek-R1-Llama-8B (MMLU)\":  \"data/mmlu/DeepSeek-R1-Distill-Llama-8B/induced_urgency/faithfulness_results.json\",\n",
    "       \"DeepSeek-R1-Llama-8B (GSM8K)\":  \"data/gsm8k/DeepSeek-R1-Distill-Llama-8B_old/unethical_information/faithfulness_results.json\",\n",
    "       \"DeepSeek-R1-Qwen-1.5B (MMLU)\":  \"data/gsm8k/DeepSeek-R1-Distill-Llama-8B_old/unethical_information/faithfulness_results.json\",\n",
    "       \"DeepSeek-R1-Qwen-1.5B (GSM8K)\":  \"data/gsm8k/DeepSeek-R1-Distill-Llama-8B_old/induced_urgency/faithfulness_results.json\",\n",
    "    },\n",
    "    \"sycophancy\": {\n",
    "       \"DeepSeek-R1-Llama-8B (MMLU)\":  \"data/mmlu/DeepSeek-R1-Distill-Llama-8B/sycophancy/faithfulness_results.json\",\n",
    "       \"DeepSeek-R1-Llama-8B (GSM8K)\":  \"data/gsm8k/DeepSeek-R1-Distill-Llama-8B_old/sycophancy/faithfulness_results.json\",\n",
    "       \"DeepSeek-R1-Qwen-1.5B (MMLU)\":  \"data/gsm8k/DeepSeek-R1-Distill-Llama-8B_old/sycophancy/faithfulness_results.json\",\n",
    "       \"DeepSeek-R1-Qwen-1.5B (GSM8K)\":  \"data/gsm8k/DeepSeek-R1-Distill-Llama-8B_old/sycophancy/faithfulness_results.json\",\n",
    "    },\n",
    "    \"induced_urgency\": {\n",
    "       \"DeepSeek-R1-Llama-8B (MMLU)\":  \"data/mmlu/DeepSeek-R1-Distill-Llama-8B/unethical_information/faithfulness_results.json\",\n",
    "       \"DeepSeek-R1-Llama-8B (GSM8K)\":  \"data/gsm8k/DeepSeek-R1-Distill-Llama-8B_old/induced_urgency/faithfulness_results.json\",\n",
    "       \"DeepSeek-R1-Qwen-1.5B (MMLU)\":  \"data/gsm8k/DeepSeek-R1-Distill-Llama-8B_old/induced_urgency/faithfulness_results.json\",\n",
    "       \"DeepSeek-R1-Qwen-1.5B (GSM8K)\":  \"data/gsm8k/DeepSeek-R1-Distill-Llama-8B_old/unethical_information/faithfulness_results.json\",\n",
    "    }\n",
    "}\n",
    "\n",
    "plot_faithfulness(\n",
    "    hint_types=hint_types,\n",
    "    model_names=model_names,\n",
    "    paths=paths,\n",
    "    out_file=\"src/visuals/faithfulness_bar_chart.png\",\n",
    "    bar_colors=[\"#70a98b\", \"#d9a6d9\"],\n",
    "    frame_color=\"#000000\",\n",
    "    bar_width=0.1,\n",
    "    spacing_factor=3.5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
