{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/ThinkLogits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/ThinkLogits/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "\n",
    "from src.main.pipeline import load_model_and_tokenizer, generate_dataset_completions\n",
    "from src.eval.llm_verificator import run_verification\n",
    "from src.eval.switch_check import run_switch_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 15:53:55,117 - INFO - CUDA is available. Using GPU.\n",
      "2025-04-14 15:53:55,118 - INFO - Loading model and tokenizer: deepseek-ai/DeepSeek-R1-Distill-Llama-8B onto cuda\n",
      "Fetching 2 files: 100%|██████████| 2/2 [00:41<00:00, 20.84s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 49.74it/s]\n",
      "2025-04-14 15:54:45,073 - INFO - Model and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "model, tokenizer, model_name, device = load_model_and_tokenizer(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hint_types = [\"none\", \"sycophancy\", \"induced_urgency\", \"unethical_information\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 15:55:09,343 - INFO - Using chat template: <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "2025-04-14 15:55:09,344 - INFO - --- Processing dataset for hint type: none ---\n",
      "2025-04-14 15:55:09,364 - ERROR - Data file not found: data/none/hints.json\n",
      "2025-04-14 15:55:09,365 - INFO - Generating completions for none...\n",
      "2025-04-14 15:55:09,366 - INFO - Using max_new_tokens: 2048\n",
      "2025-04-14 15:55:09,366 - INFO - Processing batch 1/30 (Size: 5, QIDs: 0-4)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 15:55:23,060 - INFO - Processing batch 2/30 (Size: 5, QIDs: 5-9)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 15:55:49,858 - INFO - Processing batch 3/30 (Size: 5, QIDs: 10-14)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 15:56:39,401 - INFO - Processing batch 4/30 (Size: 5, QIDs: 15-19)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 15:57:29,080 - INFO - Processing batch 5/30 (Size: 5, QIDs: 20-24)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 15:57:43,937 - INFO - Processing batch 6/30 (Size: 5, QIDs: 25-29)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 15:58:08,369 - INFO - Processing batch 7/30 (Size: 5, QIDs: 30-34)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 15:58:57,977 - INFO - Processing batch 8/30 (Size: 5, QIDs: 35-39)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 15:59:35,263 - INFO - Processing batch 9/30 (Size: 5, QIDs: 40-44)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 15:59:57,677 - INFO - Processing batch 10/30 (Size: 5, QIDs: 45-49)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:00:09,156 - INFO - Processing batch 11/30 (Size: 5, QIDs: 50-54)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:00:22,642 - INFO - Processing batch 12/30 (Size: 5, QIDs: 55-59)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:01:08,877 - INFO - Processing batch 13/30 (Size: 5, QIDs: 60-64)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:01:58,278 - INFO - Processing batch 14/30 (Size: 5, QIDs: 65-69)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:02:29,088 - INFO - Processing batch 15/30 (Size: 5, QIDs: 70-74)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:02:45,820 - INFO - Processing batch 16/30 (Size: 5, QIDs: 75-79)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:03:35,718 - INFO - Processing batch 17/30 (Size: 5, QIDs: 80-84)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:03:48,012 - INFO - Processing batch 18/30 (Size: 5, QIDs: 85-89)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:04:17,783 - INFO - Processing batch 19/30 (Size: 5, QIDs: 90-94)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:04:31,619 - INFO - Processing batch 20/30 (Size: 5, QIDs: 95-99)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:04:55,919 - INFO - Processing batch 21/30 (Size: 5, QIDs: 100-104)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:05:19,247 - INFO - Processing batch 22/30 (Size: 5, QIDs: 105-109)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:06:08,625 - INFO - Processing batch 23/30 (Size: 5, QIDs: 110-114)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:06:22,263 - INFO - Processing batch 24/30 (Size: 5, QIDs: 115-119)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:07:11,644 - INFO - Processing batch 25/30 (Size: 5, QIDs: 120-124)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:07:32,340 - INFO - Processing batch 26/30 (Size: 5, QIDs: 125-129)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:07:50,375 - INFO - Processing batch 27/30 (Size: 5, QIDs: 130-134)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:08:09,022 - INFO - Processing batch 28/30 (Size: 5, QIDs: 135-139)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:08:32,529 - INFO - Processing batch 29/30 (Size: 5, QIDs: 140-144)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:09:05,908 - INFO - Processing batch 30/30 (Size: 5, QIDs: 145-149)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:09:24,338 - INFO - Results saved to data/none/completions_DeepSeek-R1-Distill-Llama-8B_with_150.json\n",
      "2025-04-14 16:09:24,339 - INFO - Total processing time: 855.00 seconds\n"
     ]
    }
   ],
   "source": [
    "generate_dataset_completions(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    model_name = model_name,\n",
    "    device = device,\n",
    "    hint_types = hint_types\n",
    "    batch_size = 5,\n",
    "    max_new_tokens = None,\n",
    "    n_questions = 150\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run llm verification to get the final model answers\n",
    "# Note that this will drop the results that are N/A (eg the model never stopped reasoning)\n",
    "run_verification(hint_types, model_name, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ground truth...\n",
      "Loading base answers (none)...\n",
      "Base (none) Accuracy: 140/145 (96.55%)\n",
      "\n",
      "Processing hint type: sycophancy...\n",
      "Analyzing switches for sycophancy against none...\n",
      "Analysis complete for sycophancy. Found 142 entries.\n",
      "\n",
      "Processing hint type: induced_urgency...\n",
      "Analyzing switches for induced_urgency against none...\n",
      "Analysis complete for induced_urgency. Found 143 entries.\n",
      "\n",
      "Processing hint type: unethical_information...\n",
      "Analyzing switches for unethical_information against none...\n",
      "Analysis complete for unethical_information. Found 140 entries.\n",
      "\n",
      "--- Overall Results ---\n",
      "\n",
      "Hint Type: sycophancy\n",
      "  Total Entries: 142\n",
      "  Switched Answers: 6 (4.23%)\n",
      "  Switched to Correct Answer: 5 (3.52%)\n",
      "\n",
      "Hint Type: induced_urgency\n",
      "  Total Entries: 143\n",
      "  Switched Answers: 6 (4.20%)\n",
      "  Switched to Correct Answer: 4 (2.80%)\n",
      "\n",
      "Hint Type: unethical_information\n",
      "  Total Entries: 140\n",
      "  Switched Answers: 7 (5.00%)\n",
      "  Switched to Correct Answer: 5 (3.57%)\n",
      "\n",
      "Results saved to data/switch_analysis_DeepSeek-R1-Distill-Llama-8B_with_150.json\n"
     ]
    }
   ],
   "source": [
    "# Check if the model switches between none and the other hint types\n",
    "run_switch_check(hint_types[1:], model_name, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 12:38:06,964 - INFO - Using chat template: <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "2025-04-14 12:38:06,966 - INFO - --- Processing dataset for hint type: unethical_information ---\n",
      "2025-04-14 12:38:06,997 - INFO - Generating completions for unethical_information...\n",
      "2025-04-14 12:38:06,998 - INFO - Using max_new_tokens: 2048\n",
      "2025-04-14 12:38:06,999 - INFO - Processing batch 1/30 (Size: 5, QIDs: 0-4)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 12:38:56,166 - INFO - Processing batch 2/30 (Size: 5, QIDs: 5-9)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 12:39:19,342 - INFO - Processing batch 3/30 (Size: 5, QIDs: 10-14)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 12:40:32,562 - INFO - Processing batch 4/30 (Size: 5, QIDs: 15-19)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 12:41:45,620 - INFO - Processing batch 5/30 (Size: 5, QIDs: 20-24)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 12:42:58,668 - INFO - Processing batch 6/30 (Size: 5, QIDs: 25-29)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 12:43:26,047 - INFO - Processing batch 7/30 (Size: 5, QIDs: 30-34)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 12:44:25,456 - INFO - Processing batch 8/30 (Size: 5, QIDs: 35-39)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 12:45:05,853 - INFO - Processing batch 9/30 (Size: 5, QIDs: 40-44)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 12:45:30,678 - INFO - Processing batch 10/30 (Size: 5, QIDs: 45-49)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 12:45:51,033 - INFO - Processing batch 11/30 (Size: 5, QIDs: 50-54)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 12:47:04,031 - INFO - Processing batch 12/30 (Size: 5, QIDs: 55-59)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 12:47:28,689 - INFO - Processing batch 13/30 (Size: 5, QIDs: 60-64)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 12:48:41,965 - INFO - Processing batch 14/30 (Size: 5, QIDs: 65-69)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 12:49:11,435 - INFO - Processing batch 15/30 (Size: 5, QIDs: 70-74)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 12:49:29,741 - INFO - Processing batch 16/30 (Size: 5, QIDs: 75-79)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 12:50:42,493 - INFO - Processing batch 17/30 (Size: 5, QIDs: 80-84)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 12:51:05,019 - INFO - Processing batch 18/30 (Size: 5, QIDs: 85-89)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 12:52:17,818 - INFO - Processing batch 19/30 (Size: 5, QIDs: 90-94)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 12:53:08,559 - INFO - Processing batch 20/30 (Size: 5, QIDs: 95-99)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 12:54:21,784 - INFO - Processing batch 21/30 (Size: 5, QIDs: 100-104)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 12:55:13,232 - INFO - Processing batch 22/30 (Size: 5, QIDs: 105-109)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 12:56:26,401 - INFO - Processing batch 23/30 (Size: 5, QIDs: 110-114)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 12:56:44,824 - INFO - Processing batch 24/30 (Size: 5, QIDs: 115-119)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 12:57:58,014 - INFO - Processing batch 25/30 (Size: 5, QIDs: 120-124)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 12:58:36,042 - INFO - Processing batch 26/30 (Size: 5, QIDs: 125-129)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 12:59:09,454 - INFO - Processing batch 27/30 (Size: 5, QIDs: 130-134)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 12:59:30,851 - INFO - Processing batch 28/30 (Size: 5, QIDs: 135-139)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 13:00:06,218 - INFO - Processing batch 29/30 (Size: 5, QIDs: 140-144)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 13:01:19,464 - INFO - Processing batch 30/30 (Size: 5, QIDs: 145-149)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 13:01:52,400 - INFO - Results saved to data/unethical_information/completions_DeepSeek-R1-Distill-Llama-8B_with_150.json\n",
      "2025-04-14 13:01:52,401 - INFO - Total processing time: 1425.44 seconds\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from src.eval.faithfulness_metric import compute_faithfulness_score\n",
    "\n",
    "#    Each entry must have \"question_id\", \"final_answer\"\n",
    "#    The hinted set also needs \"hint_label\" and \"completion\" with the chain-of-thought text\n",
    "unhinted_path = \"data/induced_urgency/completions_DeepSeek-R1-Distill-Llama-8B_with_150.json\"\n",
    "hinted_path   = \"/root/ThinkLogits/data/sycophancy/completions_DeepSeek-R1-Distill-Llama-8B_with_150.json\"\n",
    "\n",
    "with open(unhinted_path, \"r\") as f:\n",
    "    unhinted_completions = json.load(f)\n",
    "\n",
    "with open(hinted_path, \"r\") as f:\n",
    "    hinted_completions = json.load(f)\n",
    "\n",
    "# Compute the faithfulness score (no random correction):\n",
    "faithfulness_raw = compute_faithfulness_score(\n",
    "    unhinted_data=unhinted_completions,\n",
    "    hinted_data=hinted_completions,\n",
    "    random_baseline_correction=False\n",
    ")\n",
    "print(\"Raw Faithfulness (no random correction):\", faithfulness_raw)\n",
    "\n",
    "# Compute the faithfulness score (with random-baseline correction):\n",
    "faithfulness_corrected = compute_faithfulness_score(\n",
    "    unhinted_data=unhinted_completions,\n",
    "    hinted_data=hinted_completions,\n",
    "    random_baseline_correction=True\n",
    ")\n",
    "print(\"Faithfulness (with random correction):\", faithfulness_corrected)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
