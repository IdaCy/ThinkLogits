{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/ThinkLogits\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/ThinkLogits/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "\n",
    "from src.main.pipeline import load_model_and_tokenizer, generate_dataset_completions\n",
    "from src.eval.llm_verificator import run_verification\n",
    "from src.eval.switch_check import run_switch_check\n",
    "from src.eval.llm_hint_verificator import run_hint_verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 17:31:43,715 - INFO - CUDA is available. Using GPU.\n",
      "2025-04-14 17:31:43,716 - INFO - Loading model and tokenizer: deepseek-ai/DeepSeek-R1-Distill-Llama-8B onto cuda\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 47.94it/s]\n",
      "2025-04-14 17:31:48,819 - INFO - Model and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "model, tokenizer, model_name, device = load_model_and_tokenizer(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hint_types = [\"none\", \"sycophancy\", \"induced_urgency\", \"unethical_information\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 15:55:09,343 - INFO - Using chat template: <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "2025-04-14 15:55:09,344 - INFO - --- Processing dataset for hint type: none ---\n",
      "2025-04-14 15:55:09,364 - ERROR - Data file not found: data/none/hints.json\n",
      "2025-04-14 15:55:09,365 - INFO - Generating completions for none...\n",
      "2025-04-14 15:55:09,366 - INFO - Using max_new_tokens: 2048\n",
      "2025-04-14 15:55:09,366 - INFO - Processing batch 1/30 (Size: 5, QIDs: 0-4)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 15:55:23,060 - INFO - Processing batch 2/30 (Size: 5, QIDs: 5-9)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 15:55:49,858 - INFO - Processing batch 3/30 (Size: 5, QIDs: 10-14)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 15:56:39,401 - INFO - Processing batch 4/30 (Size: 5, QIDs: 15-19)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 15:57:29,080 - INFO - Processing batch 5/30 (Size: 5, QIDs: 20-24)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 15:57:43,937 - INFO - Processing batch 6/30 (Size: 5, QIDs: 25-29)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 15:58:08,369 - INFO - Processing batch 7/30 (Size: 5, QIDs: 30-34)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 15:58:57,977 - INFO - Processing batch 8/30 (Size: 5, QIDs: 35-39)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 15:59:35,263 - INFO - Processing batch 9/30 (Size: 5, QIDs: 40-44)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 15:59:57,677 - INFO - Processing batch 10/30 (Size: 5, QIDs: 45-49)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:00:09,156 - INFO - Processing batch 11/30 (Size: 5, QIDs: 50-54)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:00:22,642 - INFO - Processing batch 12/30 (Size: 5, QIDs: 55-59)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:01:08,877 - INFO - Processing batch 13/30 (Size: 5, QIDs: 60-64)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:01:58,278 - INFO - Processing batch 14/30 (Size: 5, QIDs: 65-69)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:02:29,088 - INFO - Processing batch 15/30 (Size: 5, QIDs: 70-74)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:02:45,820 - INFO - Processing batch 16/30 (Size: 5, QIDs: 75-79)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:03:35,718 - INFO - Processing batch 17/30 (Size: 5, QIDs: 80-84)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:03:48,012 - INFO - Processing batch 18/30 (Size: 5, QIDs: 85-89)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:04:17,783 - INFO - Processing batch 19/30 (Size: 5, QIDs: 90-94)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:04:31,619 - INFO - Processing batch 20/30 (Size: 5, QIDs: 95-99)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:04:55,919 - INFO - Processing batch 21/30 (Size: 5, QIDs: 100-104)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:05:19,247 - INFO - Processing batch 22/30 (Size: 5, QIDs: 105-109)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:06:08,625 - INFO - Processing batch 23/30 (Size: 5, QIDs: 110-114)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:06:22,263 - INFO - Processing batch 24/30 (Size: 5, QIDs: 115-119)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:07:11,644 - INFO - Processing batch 25/30 (Size: 5, QIDs: 120-124)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:07:32,340 - INFO - Processing batch 26/30 (Size: 5, QIDs: 125-129)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:07:50,375 - INFO - Processing batch 27/30 (Size: 5, QIDs: 130-134)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:08:09,022 - INFO - Processing batch 28/30 (Size: 5, QIDs: 135-139)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:08:32,529 - INFO - Processing batch 29/30 (Size: 5, QIDs: 140-144)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:09:05,908 - INFO - Processing batch 30/30 (Size: 5, QIDs: 145-149)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-04-14 16:09:24,338 - INFO - Results saved to data/none/completions_DeepSeek-R1-Distill-Llama-8B_with_150.json\n",
      "2025-04-14 16:09:24,339 - INFO - Total processing time: 855.00 seconds\n"
     ]
    }
   ],
   "source": [
    "generate_dataset_completions(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    model_name = model_name,\n",
    "    device = device,\n",
    "    hint_types = hint_types\n",
    "    batch_size = 5,\n",
    "    max_new_tokens = None,\n",
    "    n_questions = 150\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run llm verification to get the final model answers\n",
    "# Note that this will drop the results that are N/A (eg the model never stopped reasoning)\n",
    "run_verification(hint_types, model_name, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ground truth...\n",
      "Loading base answers (none)...\n",
      "Base (none) Accuracy: 140/145 (96.55%)\n",
      "\n",
      "Processing hint type: sycophancy...\n",
      "Analyzing switches for sycophancy against none...\n",
      "Analysis complete for sycophancy. Found 142 entries.\n",
      "\n",
      "Processing hint type: induced_urgency...\n",
      "Analyzing switches for induced_urgency against none...\n",
      "Analysis complete for induced_urgency. Found 143 entries.\n",
      "\n",
      "Processing hint type: unethical_information...\n",
      "Analyzing switches for unethical_information against none...\n",
      "Analysis complete for unethical_information. Found 140 entries.\n",
      "\n",
      "--- Overall Results ---\n",
      "\n",
      "Hint Type: sycophancy\n",
      "  Total Entries: 142\n",
      "  Switched Answers: 6 (4.23%)\n",
      "  Switched to Correct Answer: 5 (3.52%)\n",
      "\n",
      "Hint Type: induced_urgency\n",
      "  Total Entries: 143\n",
      "  Switched Answers: 6 (4.20%)\n",
      "  Switched to Correct Answer: 4 (2.80%)\n",
      "\n",
      "Hint Type: unethical_information\n",
      "  Total Entries: 140\n",
      "  Switched Answers: 7 (5.00%)\n",
      "  Switched to Correct Answer: 5 (3.57%)\n",
      "\n",
      "Results saved to data/switch_analysis_DeepSeek-R1-Distill-Llama-8B_with_150.json\n"
     ]
    }
   ],
   "source": [
    "# Check if the model switches between none and the other hint types\n",
    "run_switch_check(hint_types[1:], model_name, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running verification for induced_urgency...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verifying induced_urgency completions:   0%|          | 0/4 [00:00<?, ?it/s]2025-04-14 17:37:55,461 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-14 17:37:56,469 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-14 17:37:56,471 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  25%|██▌       | 1/4 [00:01<00:03,  1.01s/it]2025-04-14 17:37:56,472 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-14 17:37:57,258 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-14 17:37:57,260 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  50%|█████     | 2/4 [00:01<00:01,  1.14it/s]2025-04-14 17:37:57,261 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-14 17:37:58,114 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-14 17:37:58,116 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions:  75%|███████▌  | 3/4 [00:02<00:00,  1.15it/s]2025-04-14 17:37:58,117 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-14 17:37:59,148 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-14 17:37:59,150 - INFO - AFC remote call 1 is done.\n",
      "Verifying induced_urgency completions: 100%|██████████| 4/4 [00:03<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running verification for unethical_information...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verifying unethical_information completions:   0%|          | 0/5 [00:00<?, ?it/s]2025-04-14 17:37:59,155 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-14 17:37:59,922 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-14 17:37:59,924 - INFO - AFC remote call 1 is done.\n",
      "Verifying unethical_information completions:  20%|██        | 1/5 [00:00<00:03,  1.30it/s]2025-04-14 17:37:59,924 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-14 17:38:00,747 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-14 17:38:00,749 - INFO - AFC remote call 1 is done.\n",
      "Verifying unethical_information completions:  40%|████      | 2/5 [00:01<00:02,  1.25it/s]2025-04-14 17:38:00,750 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-14 17:38:01,706 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-14 17:38:01,708 - INFO - AFC remote call 1 is done.\n",
      "Verifying unethical_information completions:  60%|██████    | 3/5 [00:02<00:01,  1.14it/s]2025-04-14 17:38:01,709 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-14 17:38:02,786 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-14 17:38:02,787 - INFO - AFC remote call 1 is done.\n",
      "Verifying unethical_information completions:  80%|████████  | 4/5 [00:03<00:00,  1.05it/s]2025-04-14 17:38:02,788 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-04-14 17:38:03,837 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-04-14 17:38:03,839 - INFO - AFC remote call 1 is done.\n",
      "Verifying unethical_information completions: 100%|██████████| 5/5 [00:04<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# Verify if the model verbalizes the hint\n",
    "run_hint_verification(hint_types[1:], model_name, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from src.eval.faithfulness_metric import compute_faithfulness_score\n",
    "\n",
    "#    Each entry must have \"question_id\", \"final_answer\"\n",
    "#    The hinted set also needs \"hint_label\" and \"completion\" with the chain-of-thought text\n",
    "unhinted_path = \"data/induced_urgency/completions_DeepSeek-R1-Distill-Llama-8B_with_150.json\"\n",
    "hinted_path   = \"/root/ThinkLogits/data/sycophancy/completions_DeepSeek-R1-Distill-Llama-8B_with_150.json\"\n",
    "\n",
    "with open(unhinted_path, \"r\") as f:\n",
    "    unhinted_completions = json.load(f)\n",
    "\n",
    "with open(hinted_path, \"r\") as f:\n",
    "    hinted_completions = json.load(f)\n",
    "\n",
    "# Compute the faithfulness score (no random correction):\n",
    "faithfulness_raw = compute_faithfulness_score(\n",
    "    unhinted_data=unhinted_completions,\n",
    "    hinted_data=hinted_completions,\n",
    "    random_baseline_correction=False\n",
    ")\n",
    "print(\"Raw Faithfulness (no random correction):\", faithfulness_raw)\n",
    "\n",
    "# Compute the faithfulness score (with random-baseline correction):\n",
    "faithfulness_corrected = compute_faithfulness_score(\n",
    "    unhinted_data=unhinted_completions,\n",
    "    hinted_data=hinted_completions,\n",
    "    random_baseline_correction=True\n",
    ")\n",
    "print(\"Faithfulness (with random correction):\", faithfulness_corrected)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
