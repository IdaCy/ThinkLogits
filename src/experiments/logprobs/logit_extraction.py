"""Core functions for extracting log probabilities during CoT reasoning."""

import logging 
import torch # Assuming Hugging Face models return torch tensors
import math
from tqdm import tqdm
import re # For case-insensitive search

def get_option_token_ids(options: list[str], tokenizer) -> dict[str, int]:
    """Maps option labels (e.g., 'A', 'B') to their token IDs.

    Assumes standard MCQ options where the relevant token includes a leading space
    (e.g., it looks for the token ID of ' A', not 'A').

    Args:
        options: A list of option labels (e.g., ['A', 'B', 'C', 'D']).
        tokenizer: The Hugging Face tokenizer instance.

    Returns:
        A dictionary mapping the option label (str) to its token ID (int).
        Returns an empty dict if tokenizer is missing or some options are not found.
    """
    option_token_ids = {}
    if not tokenizer:
        logging.error("Tokenizer not provided to get_option_token_ids")
        return option_token_ids

    missing_options = []

    for option_label in options:
        option_with_space = f" {option_label}" # e.g., " A"
        token_id = tokenizer.encode(option_with_space, add_special_tokens=False)[0]

        if token_id is not None:
            option_token_ids[option_label] = token_id
        else:
            logging.warning(f"Could not find a unique token ID for option '{option_with_space}'.")
            missing_options.append(option_label)

    if missing_options:
        logging.warning(f"Could not map all option labels to token IDs. Missing: {missing_options}")

    return option_token_ids

DEFAULT_REASONING_START_MARKER = "assistant" # Look for assistant marker

def find_reasoning_start(completion_text: str, model_name: str = None) -> int:
    """Finds the character index marking the start of the actual reasoning.

    Searches case-insensitively for the end of the assistant marker.

    Args:
        completion_text: The full text generated by the model.
        model_name: The name of the model (optional, for future customization).

    Returns:
        The character index where the reasoning is assumed to start.
        Returns 0 if the marker is not found.
    """
    # TODO: Implement more sophisticated model-specific logic if needed
    #       e.g., look for specific template tokens like <|end_header_id|>\n
    marker = DEFAULT_REASONING_START_MARKER
    try:
        index = completion_text.lower().rfind(marker) # Search
        if index != -1:
            logging.debug(f"Found reasoning start marker '{marker}' at index {index}.")
            return index
        else:
            logging.warning(f"Reasoning start marker '{marker}' not found in completion. Using full length.")
            return 0    
    except Exception as e:
        logging.error(f"Error finding reasoning start marker: {e}")
        return 0 # Fallback on error

DEFAULT_REASONING_END_MARKER = "</think>"

def find_reasoning_end(completion_text: str, model_name: str = None) -> int:
    """Finds the character index marking the end of the CoT reasoning.

    Currently searches for a default marker ("Final Answer:").
    Can be extended based on model_name if needed.

    Args:
        completion_text: The full text generated by the model.
        model_name: The name of the model (optional, for future customization).

    Returns:
        The character index where the reasoning is assumed to end (start of the marker).
        Returns the length of the text if the marker is not found.
    """
    # TODO: Implement more sophisticated model-specific logic if needed
    marker = DEFAULT_REASONING_END_MARKER

    try:
        index = completion_text.rfind(marker) # Search from the end
        if index != -1:
            logging.debug(f"Found reasoning end marker '{marker}' at index {index}.")
            return index
        else:
            logging.warning(f"Reasoning end marker '{marker}' not found in completion. Using full length.")
            # Fallback: Check for just the options pattern like \n(A) ... \n(B) ...
            # or perhaps the second <\\think> token as mentioned in notes?
            # For now, return full length as a simple fallback.
            return len(completion_text)
    except Exception as e:
        logging.error(f"Error finding reasoning end marker: {e}")
        return len(completion_text) # Fallback on error 

def extract_logprobs_sequence(
    completion_text: str,
    reasoning_start_index: int,
    reasoning_end_index: int,
    option_token_ids: dict[str, int], # Map from option label (e.g., 'A') to token ID
    intervention_prompt: str,
    percentage_steps: list[int],
    model,
    tokenizer,
    device: str # e.g., 'cuda' or 'cpu'
) -> list[dict]:
    """Extracts log probabilities for specified options at different CoT steps.

    Args:
        completion_text: The full completion text from the model.
        reasoning_start_index: The character index where reasoning starts.
        reasoning_end_index: The character index where reasoning ends.
        option_token_ids: Dict mapping option labels ('A', 'B',..) to their token IDs.
        intervention_prompt: The text to append to the truncated reasoning.
        percentage_steps: List of percentages (1-100) of reasoning to analyze.
        model: The loaded Hugging Face model.
        tokenizer: The loaded Hugging Face tokenizer.
        device: The device the model is on ('cuda' or 'cpu').

    Returns:
        A list of dictionaries, each containing step info and extracted logprobs.
        Returns empty list on error or if reasoning part is empty.
    """
    logprobs_sequence = []
    if not completion_text or reasoning_end_index <= reasoning_start_index or not option_token_ids:
        logging.warning(f"Invalid start/end indices ({reasoning_start_index}/{reasoning_end_index}) or missing data for logit extraction. Returning empty list.")
        return logprobs_sequence

    # --- Tokenize parts ONCE --- 
    # 1. Prompt Context
    prompt_context_text = completion_text[:reasoning_start_index]
    prompt_context_tokens = tokenizer.encode(prompt_context_text, return_tensors="pt", add_special_tokens=False).to(device)
    # Note: Check if BOS token is needed. If the original input had BOS, encode might need add_special_tokens=True for this part?
    # Or maybe the model call handles it? Let's assume False is okay for now.
    logging.debug(f"Prompt context tokens shape: {prompt_context_tokens.shape}")

    # 2. Reasoning Part
    reasoning_text = completion_text[reasoning_start_index:reasoning_end_index]
    if not reasoning_text.strip(): # Check if reasoning is just whitespace
        logging.warning("Reasoning text (after slicing) is empty or whitespace. Returning empty list.")
        return logprobs_sequence
    all_reasoning_tokens = tokenizer.encode(reasoning_text, return_tensors="pt", add_special_tokens=False).to(device)
    total_reasoning_tokens = all_reasoning_tokens.shape[1]
    if total_reasoning_tokens == 0:
        logging.warning("Reasoning text tokenized to zero tokens. Returning empty list.")
        return logprobs_sequence
    logging.debug(f"All reasoning tokens shape: {all_reasoning_tokens.shape}")

    # 3. Intervention Prompt
    intervention_tokens = tokenizer.encode(intervention_prompt, return_tensors="pt", add_special_tokens=False).to(device)
    logging.debug(f"Intervention tokens shape: {intervention_tokens.shape}")
    # --- End Tokenization --- 

    # Ensure percentage steps are sorted and unique
    sorted_steps = sorted(list(set(step for step in percentage_steps if 1 <= step <= 100)))

    logging.info(f"Analyzing {total_reasoning_tokens} reasoning tokens across {len(sorted_steps)} steps.")

    for step_percentage in sorted_steps:
        token_index = math.ceil((step_percentage / 100) * total_reasoning_tokens)
        token_index = max(1, min(token_index, total_reasoning_tokens))

        # Get prefix of the reasoning tokens
        current_reasoning_prefix_tokens = all_reasoning_tokens[:, :token_index]

        # Combine prompt context + reasoning prefix + intervention
        input_parts = [prompt_context_tokens, current_reasoning_prefix_tokens]
        if intervention_tokens.shape[1] > 0:
            input_parts.append(intervention_tokens)
        
        input_ids = torch.cat(input_parts, dim=1)
        # logging.debug(f"Sequence going into the model: {tokenizer.decode(input_ids[0], skip_special_tokens=False)}\n\n")

        step_logprobs = {}
        try:
            # Get model outputs (logits) without calculating gradients
            with torch.no_grad():
                outputs = model(input_ids=input_ids)
                # Logits for the *next* token prediction are typically the last slice
                next_token_logits = outputs.logits[:, -1, :] # Shape: [batch_size, vocab_size]

            # Extract logits for the target option tokens
            # Assumes batch_size=1, so we take the first element [0]
            for option_label, token_id in option_token_ids.items():
                if 0 <= token_id < next_token_logits.shape[1]:
                    logit_value = next_token_logits[0, token_id].item()
                    step_logprobs[option_label] = logit_value
                else:
                    logging.warning(f"Token ID {token_id} for option '{option_label}' is out of bounds for vocab size {next_token_logits.shape[1]}. Skipping.")
                    step_logprobs[option_label] = None # Or some indicator like float('-inf')

        except Exception as e:
            logging.error(f"Error during model forward pass or logit extraction at step {step_percentage}% (token {token_index}): {e}")
            # Add placeholder for this step's logprobs on error?
            for option_label in option_token_ids.keys():
                 step_logprobs[option_label] = None

        logprobs_sequence.append({
            "step_percentage": step_percentage,
            "token_index": token_index,
            "logprobs": step_logprobs
        })

    return logprobs_sequence 