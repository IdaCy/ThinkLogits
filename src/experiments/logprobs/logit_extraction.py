"""Core functions for extracting log probabilities during CoT reasoning."""

import logging 
import torch # Assuming Hugging Face models return torch tensors
import math
from tqdm import tqdm

def get_option_token_ids(options: list[str], tokenizer) -> dict[str, int]:
    """Maps option labels (e.g., 'A', 'B') to their token IDs.

    Assumes standard MCQ options where the relevant token includes a leading space
    (e.g., it looks for the token ID of ' A', not 'A').

    Args:
        options: A list of option labels (e.g., ['A', 'B', 'C', 'D']).
        tokenizer: The Hugging Face tokenizer instance.

    Returns:
        A dictionary mapping the option label (str) to its token ID (int).
        Returns an empty dict if tokenizer is missing or some options are not found.
    """
    option_token_ids = {}
    if not tokenizer:
        logging.error("Tokenizer not provided to get_option_token_ids")
        return option_token_ids

    missing_options = []

    for option_label in options:
        option_with_space = f" {option_label}" # e.g., " A"
        token_id = tokenizer.encode(option_with_space, add_special_tokens=False)[0]

        if token_id is not None:
            option_token_ids[option_label] = token_id
        else:
            logging.warning(f"Could not find a unique token ID for option '{option_with_space}'.")
            missing_options.append(option_label)

    if missing_options:
        logging.warning(f"Could not map all option labels to token IDs. Missing: {missing_options}")

    return option_token_ids

DEFAULT_REASONING_END_MARKER = "</think>"

def find_reasoning_end(completion_text: str, model_name: str = None) -> int:
    """Finds the character index marking the end of the CoT reasoning.

    Currently searches for a default marker ("Final Answer:").
    Can be extended based on model_name if needed.

    Args:
        completion_text: The full text generated by the model.
        model_name: The name of the model (optional, for future customization).

    Returns:
        The character index where the reasoning is assumed to end (start of the marker).
        Returns the length of the text if the marker is not found.
    """
    # TODO: Implement more sophisticated model-specific logic if needed
    marker = DEFAULT_REASONING_END_MARKER

    try:
        index = completion_text.rfind(marker) # Search from the end
        if index != -1:
            logging.debug(f"Found reasoning end marker '{marker}' at index {index}.")
            return index
        else:
            logging.warning(f"Reasoning end marker '{marker}' not found in completion. Using full length.")
            # Fallback: Check for just the options pattern like \n(A) ... \n(B) ...
            # or perhaps the second <\\think> token as mentioned in notes?
            # For now, return full length as a simple fallback.
            return len(completion_text)
    except Exception as e:
        logging.error(f"Error finding reasoning end marker: {e}")
        return len(completion_text) # Fallback on error 

def extract_logprobs_sequence(
    completion_text: str,
    reasoning_end_index: int,
    option_token_ids: dict[str, int], # Map from option label (e.g., 'A') to token ID
    intervention_prompt: str,
    percentage_steps: list[int],
    model,
    tokenizer,
    device: str # e.g., 'cuda' or 'cpu'
) -> list[dict]:
    """Extracts log probabilities for specified options at different CoT steps.

    Args:
        completion_text: The full completion text from the model.
        reasoning_end_index: The character index where reasoning ends.
        option_token_ids: Dict mapping option labels ('A', 'B',..) to their token IDs.
        intervention_prompt: The text to append to the truncated reasoning.
        percentage_steps: List of percentages (1-100) of reasoning to analyze.
        model: The loaded Hugging Face model.
        tokenizer: The loaded Hugging Face tokenizer.
        device: The device the model is on ('cuda' or 'cpu').

    Returns:
        A list of dictionaries, each containing step info and extracted logprobs.
        Returns empty list on error or if reasoning part is empty.
    """
    logprobs_sequence = []
    if not completion_text or reasoning_end_index <= 0 or not option_token_ids:
        logging.warning("Missing necessary data for logit extraction. Returning empty list.")
        return logprobs_sequence

    reasoning_text = completion_text[:reasoning_end_index]
    if not reasoning_text.strip(): # Check if reasoning is just whitespace
        logging.warning("Reasoning text is empty or whitespace. Returning empty list.")
        return logprobs_sequence

    # Tokenize the reasoning part ONCE
    # Use return_tensors='pt' if model expects PyTorch tensors
    # Keep attention mask if model requires it
    reasoning_tokens = tokenizer.encode(reasoning_text, return_tensors="pt", add_special_tokens=False).to(device)
    # Note: Depending on the model/tokenizer, might need add_special_tokens=True for BOS?
    # Let's start with False and adjust if needed.
    total_reasoning_tokens = reasoning_tokens.shape[1]

    if total_reasoning_tokens == 0:
        logging.warning("Reasoning text tokenized to zero tokens. Returning empty list.")
        return logprobs_sequence

    # Tokenize intervention prompt
    intervention_tokens = tokenizer.encode(intervention_prompt, return_tensors="pt", add_special_tokens=False).to(device)

    # Ensure percentage steps are sorted and unique
    sorted_steps = sorted(list(set(step for step in percentage_steps if 1 <= step <= 100)))

    logging.info(f"Analyzing {total_reasoning_tokens} reasoning tokens across {len(sorted_steps)} steps.")

    # Use tqdm for progress bar if running many steps/questions
    # Wrap sorted_steps with tqdm if desired: for step_percentage in tqdm(sorted_steps, desc="Extracting Logprobs"):
    for step_percentage in sorted_steps:
        token_index = math.ceil((step_percentage / 100) * total_reasoning_tokens)
        # Ensure we take at least one token, and not more than available
        token_index = max(1, min(token_index, total_reasoning_tokens))

        # Get prefix tokens
        prefix_tokens = reasoning_tokens[:, :token_index]

        # Combine prefix with intervention
        # Note: Need to handle potential dimension mismatch if intervention_tokens is empty
        if intervention_tokens.shape[1] > 0:
            input_ids = torch.cat((prefix_tokens, intervention_tokens), dim=1)
        else:
            input_ids = prefix_tokens

        step_logprobs = {}
        try:
            # Get model outputs (logits) without calculating gradients
            with torch.no_grad():
                outputs = model(input_ids=input_ids)
                # Logits for the *next* token prediction are typically the last slice
                next_token_logits = outputs.logits[:, -1, :] # Shape: [batch_size, vocab_size]

            # Extract logits for the target option tokens
            # Assumes batch_size=1, so we take the first element [0]
            for option_label, token_id in option_token_ids.items():
                if 0 <= token_id < next_token_logits.shape[1]:
                    logit_value = next_token_logits[0, token_id].item()
                    step_logprobs[option_label] = logit_value
                else:
                    logging.warning(f"Token ID {token_id} for option '{option_label}' is out of bounds for vocab size {next_token_logits.shape[1]}. Skipping.")
                    step_logprobs[option_label] = None # Or some indicator like float('-inf')

        except Exception as e:
            logging.error(f"Error during model forward pass or logit extraction at step {step_percentage}% (token {token_index}): {e}")
            # Add placeholder for this step's logprobs on error?
            for option_label in option_token_ids.keys():
                 step_logprobs[option_label] = None

        logprobs_sequence.append({
            "step_percentage": step_percentage,
            "token_index": token_index,
            "logprobs": step_logprobs
        })

    return logprobs_sequence 